{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm import tqdm\nimport random, pickle, math, warnings\nimport itertools,  multiprocessing, json\n#warnings.simplefilter('ignore')\nprint(\"CPU Count: \", multiprocessing.cpu_count())\n\np = '/kaggle/input/santa-2024/sample_submission.csv'\ndf = pd.read_csv(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:47.651253Z","iopub.execute_input":"2024-12-17T05:11:47.651512Z","iopub.status.idle":"2024-12-17T05:11:49.565142Z","shell.execute_reply.started":"2024-12-17T05:11:47.651485Z","shell.execute_reply":"2024-12-17T05:11:49.564331Z"}},"outputs":[{"name":"stdout","text":"CPU Count:  96\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#https://www.kaggle.com/docs/tpu\n#https://www.kaggle.com/code/ryanholbrook/getting-started-with-tpus\n\n#!pip uninstall -y tensorflow && pip install tensorflow-cpu\n#import tensorflow as tf\n\n#print(\"Tensorflow version \" + tf.__version__)\n#AUTO = tf.data.experimental.AUTOTUNE\n# Detect TPU, return appropriate distribution strategy\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#print('Running on TPU ', tpu.master())\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n#tpu_strategy = tf.distribute.TPUStrategy(tpu)\n#print(\"REPLICAS: \", tpu_strategy.num_replicas_in_sync)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:49.566873Z","iopub.execute_input":"2024-12-17T05:11:49.567123Z","iopub.status.idle":"2024-12-17T05:11:49.570372Z","shell.execute_reply.started":"2024-12-17T05:11:49.567099Z","shell.execute_reply":"2024-12-17T05:11:49.569769Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import transformers, torch, os\nfrom math import exp\n\nDEVICE = torch.device('cpu')\nMODEL_PATH = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n\nclass PerplexityCalculator:\n    def __init__(self,):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", torch_dtype=torch.float32,)\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        self.model.eval()\n\n    #add batch and multiprocessing again for CPU/GPU:}\n    def get_perplexity(self, text: str) -> float:\n        with torch.no_grad():\n            text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n            model_inputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False,)\n            #model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n            logits = self.model(**model_inputs, use_cache=True)['logits']\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n            loss = self.loss_fct(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1))\n            sequence_loss = loss.sum() / len(loss)\n            loss_list = sequence_loss.cpu().item()\n        return exp(loss_list)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n     # define your model normally\nscorer = PerplexityCalculator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:49.571202Z","iopub.execute_input":"2024-12-17T05:11:49.571466Z","iopub.status.idle":"2024-12-17T05:12:24.203476Z","shell.execute_reply.started":"2024-12-17T05:11:49.571421Z","shell.execute_reply":"2024-12-17T05:12:24.202808Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"t = \"\"\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\nreindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament\nmagi yuletide cheer grinch carol holiday holly jingle naughty nice nutcracker polar beard ornament stocking chimney sleigh workshop gifts decorations\nsleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax carol sing chimney visit grinch naughty nice polar beard workshop nutcracker ornament stocking\nfrom and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping paper\nfrom and and as and have the in is it of not that the to we with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game give gifts gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\"\"\"\n\ndf['text'] = t.split('\\n')\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:24.204406Z","iopub.execute_input":"2024-12-17T05:12:24.204787Z","iopub.status.idle":"2024-12-17T05:13:14.993369Z","shell.execute_reply.started":"2024-12-17T05:12:24.204760Z","shell.execute_reply":"2024-12-17T05:13:14.992632Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"255.69357441222382\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    423.612476\n2    303.031473\n3    209.184454\n4     95.162854\n5     34.671069\nName: score, dtype: float64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"past = {}\n\ndef part_perm_brutem(st, start=0, end=3, skips=1, tol=1.0, smax=300):\n    global past\n    bestt = st\n    best = scorer.get_perplexity(st)\n    st = st.split(' ')\n    part = st[start:end]\n    if start>0:\n        st1 =  ' '.join(st[:start]) + ' '\n    else:\n        st1 = ''\n    if end<len(st): \n        st2 =  ' ' + ' '.join(st[end:])\n    else: \n        st2 = ''\n    p = list(itertools.permutations(part))\n    for i in range(0, len(p), skips): #removed tqdm\n        t =  st1 + ' '.join(list(p[i])) + st2\n        if t in past:\n            s = past[t]\n        else:\n            s =  scorer.get_perplexity(t)\n        #if s <= best * tol and t not in past and s<smax:\n        if s < best and t not in past:\n            print(\"New Score: \", s, t)\n            best = s\n            bestt = t\n        if t not in past:\n            past[t] = s\n    return bestt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:13:14.994479Z","iopub.execute_input":"2024-12-17T05:13:14.994800Z","iopub.status.idle":"2024-12-17T05:13:15.001920Z","shell.execute_reply.started":"2024-12-17T05:13:14.994761Z","shell.execute_reply":"2024-12-17T05:13:15.001148Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"i = 1\nbestt = df['text'][i]\nl = len(df['text'][i].split(' '))\nfor p in range(2, 3):\n    for start in range(0,l-p+1):\n        print(\"START: \", start)\n        bestt = part_perm_brutem(bestt, start, start+p, 1)\n        df.to_csv(\"submission.csv\", index=False)\n\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:13:15.002988Z","iopub.execute_input":"2024-12-17T05:13:15.003252Z","iopub.status.idle":"2024-12-17T05:14:12.582145Z","shell.execute_reply.started":"2024-12-17T05:13:15.003227Z","shell.execute_reply":"2024-12-17T05:14:12.581207Z"}},"outputs":[{"name":"stdout","text":"START:  0\nSTART:  1\nSTART:  2\nSTART:  3\nSTART:  4\nSTART:  5\nSTART:  6\nSTART:  7\nSTART:  8\nSTART:  9\nSTART:  10\nSTART:  11\nSTART:  12\nSTART:  13\nSTART:  14\nSTART:  15\nSTART:  16\nSTART:  17\nSTART:  18\n255.69357441222382\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    423.612476\n2    303.031473\n3    209.184454\n4     95.162854\n5     34.671069\nName: score, dtype: float64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}