{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":213120540,"sourceType":"kernelVersion"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook we add a crossover function to the basic genetic algorithm shown in [here](https://www.kaggle.com/code/aatiffraz/basic-genetic-search-algorithm). \n\nIt quickly converges towards the minimum of sample 1. By choosing a random position for the crossover, the algorithm escapes local minima easier, but converges at a slower rate.","metadata":{}},{"cell_type":"markdown","source":"**Versions:**\n- version 1: Implementing the crossover function\n- version 2: Reactivating mutation, extending the genpool\n- version 5: Implementing a more interesting mutation function\n- version 6: Changing the crossover function","metadata":{}},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:34:39.047548Z","iopub.execute_input":"2024-12-16T11:34:39.048502Z","iopub.status.idle":"2024-12-16T11:34:51.983879Z","shell.execute_reply.started":"2024-12-16T11:34:39.048431Z","shell.execute_reply":"2024-12-16T11:34:51.982706Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\n\nimport math\nimport time\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport transformers\nimport torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:34:51.985812Z","iopub.execute_input":"2024-12-16T11:34:51.986142Z","iopub.status.idle":"2024-12-16T11:34:55.832440Z","shell.execute_reply.started":"2024-12-16T11:34:51.986110Z","shell.execute_reply":"2024-12-16T11:34:55.831662Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Read the Data","metadata":{}},{"cell_type":"markdown","source":"We read in a currently good performing notebook.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/diminutive-effort-tpu/submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:34:55.833818Z","iopub.execute_input":"2024-12-16T11:34:55.834320Z","iopub.status.idle":"2024-12-16T11:34:55.849293Z","shell.execute_reply.started":"2024-12-16T11:34:55.834276Z","shell.execute_reply":"2024-12-16T11:34:55.848411Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Perplexity Function","metadata":{}},{"cell_type":"code","source":"# Copied from https://www.kaggle.com/code/cdeotte/brute-force-first-sample-perplexity-470\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n                \n            #quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n\n            quantization_config = transformers.BitsAndBytesConfig(\n                load_in_4bit = True,\n                bnb_4bit_quant_type = \"fp4\", #fp4 nf4\n                bnb_4bit_use_double_quant = False,\n                bnb_4bit_compute_dtype=torch.float16,\n            )\n            \n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n        #if not load_in_8bit:\n        #    self.model.to(DEVICE)  # Explicitly move the model to the device\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], batch_size: 32\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        verbose : bool, default=False\n            Display progress bar.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n\n        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n        for j in range(batches):\n            \n            a = j*batch_size\n            b = (j+1)*batch_size\n            input_batch = input_texts[a:b]\n        \n            with torch.no_grad():\n\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                    padding=True\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                label = model_inputs['input_ids']\n                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = label[..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                loss = loss.view(len(logits), -1)\n                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n                loss = torch.sum(loss, -1) / valid_length\n\n                loss_list += loss.cpu().tolist()\n\n                # Debug output\n                #print(f\"\\nProcessing: '{text}'\")\n                #print(f\"With special tokens: '{text_with_special}'\")\n                #print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                #print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                #print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                #print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                #print(f\"Individual losses: {loss.tolist()}\")\n                #print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        # print(\"\\nFinal perplexities:\")\n        # for text, perp in zip(input_texts, ppl):\n        #     print(f\"Text: '{text}'\")\n        #     print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()\n\n\nscorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-16T11:34:55.851811Z","iopub.execute_input":"2024-12-16T11:34:55.852201Z","iopub.status.idle":"2024-12-16T11:37:47.067779Z","shell.execute_reply.started":"2024-12-16T11:34:55.852161Z","shell.execute_reply":"2024-12-16T11:37:47.066837Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97dffe0071004991b4ab84fcf39e0060"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Genetic Algorithm","metadata":{}},{"cell_type":"markdown","source":"Here we define the crossover function and the genetic algorithm.","metadata":{}},{"cell_type":"markdown","source":"## Crossover Function","metadata":{}},{"cell_type":"markdown","source":"The crossover function generates two offspring from two parent sequences. It typically selects a small segment of 2 to 5 words (occasionally a larger segment) from each parent and inserts this segment into the corresponding position (or into a random position if random_position == True) in the offspring. The remaining positions in the offspring are filled with the words from the other parent, maintaining their original order.","metadata":{}},{"cell_type":"code","source":"def order_crossover(p1, p2, random_position = False, verbose = False):\n    # Split the parent texts into lists of words\n    # Ensure both parents are lists of words \n    #p1 = p1.split() \n    #p2 = p2.split()\n    size = len(p1)\n    child1, child2 = [None]*size, [None]*size\n\n    # Ensure both parents have the same number of words\n    if len(p1) != len(p2):\n        print(f\"parent 1: {p1}\")\n        print(f\"parent2: {p2}\")\n        raise ValueError(\"Both parents must have the same number of words.\")\n\n    # Randomly select a subsequence\n    if random.random() < 0.2: #in 20% of the cases a larger crossover\n        crossover_size = random.choice([2, size-1])\n    else: #random crossover\n        crossover_size = random.choice([2, 5])\n    start = random.randint(0, size - crossover_size-1) \n    end = start + crossover_size\n    if verbose:\n        print(f\"Selected subsequence length {crossover_size} from index {start} to {end-1}\")\n\n    # Copy the subsequence to the children at a random position\n    if random_position:\n        start_new = random.randint(0, size - crossover_size-1)\n        end_new = start_new + crossover_size\n    else:\n        start_new = start\n        end_new = end\n    if verbose:\n        print(f\"Selected new position is {start_new}\")\n    child1[start_new:end_new] = p1[start:end]\n    child2[start_new:end_new] = p2[start:end]\n\n    # Fill the remaining positions\n    def fill_remaining(child, parent):\n        child_temp = child.copy()\n        parent_idx = 0\n        for i in range(size):\n            if child[i] is None:\n                while parent_idx < size and parent[parent_idx] in child_temp:\n                    child_temp.remove(parent[parent_idx])\n                    parent_idx += 1\n                if parent_idx < size:\n                    child[i] = parent[parent_idx]\n                    parent_idx += 1\n\n    fill_remaining(child1, p2)\n    fill_remaining(child2, p1)\n\n    # Join the lists of words back into strings\n    #child1 = \" \".join(child1)\n    #child2 = \" \".join(child2)\n\n    return child1, child2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:37:47.069097Z","iopub.execute_input":"2024-12-16T11:37:47.069869Z","iopub.status.idle":"2024-12-16T11:37:47.079777Z","shell.execute_reply.started":"2024-12-16T11:37:47.069824Z","shell.execute_reply":"2024-12-16T11:37:47.078887Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Now just a small test that crossover is working: ","metadata":{}},{"cell_type":"code","source":"# Example parents\np1 = \"hohoho this is a merry santa crossover test in a genetic workshop\"\np2 = \"crossover workshop hohoho in a this test merry genetic is santa a\"\n\n# Perform crossover\nchild1, child2 = order_crossover(p1.split(), p2.split(), random_position = True, verbose = True)\n\nprint(\"Child1:\", ' '.join(child1))\nprint(\"Child2:\", ' '.join(child2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:37:47.080802Z","iopub.execute_input":"2024-12-16T11:37:47.081096Z","iopub.status.idle":"2024-12-16T11:37:47.093387Z","shell.execute_reply.started":"2024-12-16T11:37:47.081053Z","shell.execute_reply":"2024-12-16T11:37:47.092648Z"}},"outputs":[{"name":"stdout","text":"Selected subsequence length 2 from index 4 to 5\nSelected new position is 9\nChild1: crossover workshop hohoho in a this test genetic is merry santa a\nChild2: hohoho is merry santa crossover test in a genetic a this workshop\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Basic Genetic Algorithm","metadata":{}},{"cell_type":"markdown","source":"Now we define the Genetic Algorithm as in [this notebook](https://www.kaggle.com/code/aatiffraz/basic-genetic-search-algorithm), but add the cross-over function. We comment out some of the functionalities of original code, but we reintroduce the possibility for mutations. We define the mutation function seperately. Mutation keeps the genpool diverse and the rate can be adjusted by the parameter `mutation_rate`.","metadata":{}},{"cell_type":"code","source":"def mutate(child): \n    #mutation_type = random.choice(['swap2', 'swap3','swap', 'inversion', 'scramble', 'shuffle']) \n    mutation_type = random.choice(['swap2', 'swap3','swap', 'inversion', 'scramble']) \n    #child = child.split() # Convert child to a list of words \n    n = len(child) \n    if mutation_type == \"swap2\":\n        i,j = random.sample(range(n),2)\n        child[i], child[j] = child[j], child[i] \n    elif mutation_type == \"swap3\":\n        i,j,k = random.sample(range(n),3)\n        child[i], child[j], child[k] = child[j], child[k], child[i] \n    elif mutation_type == 'swap': \n        mutation_size = random.randint(1, n//3) \n        for _ in range(mutation_size): \n            i, j = random.sample(range(n), 2) \n            child[i], child[j] = child[j], child[i] \n    elif mutation_type == 'inversion': \n        start, end = sorted(random.sample(range(n), 2)) \n        child[start:end+1] = reversed(child[start:end+1]) \n    elif mutation_type == 'scramble': \n        start, end = sorted(random.sample(range(n), 2)) \n        subset = child[start:end+1] \n        random.shuffle(subset) \n        child[start:end+1] = subset \n    elif mutation_type == \"shuffle\":\n        random.shuffle(child)\n    return child #\" \".join(child)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:37:47.094541Z","iopub.execute_input":"2024-12-16T11:37:47.095249Z","iopub.status.idle":"2024-12-16T11:37:47.103332Z","shell.execute_reply.started":"2024-12-16T11:37:47.095218Z","shell.execute_reply":"2024-12-16T11:37:47.102455Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"I noticed that it is more practical to deal with single words instead of the full text, as it allows to keep good working word combinations together. Therefore I changed the algorithm accordingly. The one point which is not working with word combinations is the step with the set, where only unique solutions are scored. This needs to be commented out if the aim is to work with word combinations.","metadata":{}},{"cell_type":"code","source":"#copied from https://www.kaggle.com/code/aatiffraz/basic-genetic-search-algorithm\ndef genetic_search(population, scorer, iterations, population_size, top_k, mutation_rate = 0.2, \n                   random_position = False, verbose = False):\n    print(f\"Starting \\n  ------------------------------------------------------\\n\")\n    start_time = time.time()\n    words = population[0]#.split()\n    n = len(words)\n\n    #Start with a the parent population\n    #random population\n    # Large initial search population\n    #population = [\" \".join(random.sample(words, n)) for _ in range(population_size*20)]\n    #population.append(\" \".join(words))\n    population_text = [\" \".join(population[i]) for i in range(len(population))]\n\n    scores = scorer.get_perplexity(population_text, batch_size=2)\n\n    best_score = float('inf')\n    best_sequence = None\n    last_best_edited = 0\n    last_mutation = 0\n    last_best_score = float(\"inf\")\n    last_mutation = 0\n    \n    for iteration in range(iterations):\n        # Select few top sequences\n        top_indices = np.argsort(scores)[:top_k]\n        # Why should only the fittest make it? Come on, the others want to have fun too:\n        remaining_indices = np.argsort(scores)[top_k:] \n        # Add a random sample of 15 from the remaining indices \n        random_sample = random.sample(list(remaining_indices), min(top_k // 3, len(remaining_indices))) \n        top_indices = np.concatenate((top_indices, random_sample)).astype(int)\n        \n        top_sequences = [population[i] for i in top_indices]\n        top_scores = [scores[i] for i in top_indices]\n        last_best_edited += 1\n        last_mutation += 1\n\n        if top_scores[0] < best_score:\n            best_score = top_scores[0]\n            best_sequence = top_sequences[0]\n            last_best_edited = 0\n            if verbose:\n                print(f\" Best new sequence: {best_sequence}\")\n\n        print(f\"Best Scores: {top_scores[0:3]} on {iteration}th iteration in {time.time() - start_time}s\")\n        \n        new_population = []\n        for _ in range(population_size):\n            # Choose two random top sequence as a parents\n            parent1 = random.choice(top_sequences)#.split()\n            #parent1 = weighted_random_choice(population, probabilities)\n            # Select parent2 ensuring it's different from parent1 \n            while True: \n                parent2 = random.choice(top_sequences)#.split() \n                #parent2 = weighted_random_choice(population, probabilities)\n                if parent2 != parent1:\n                    break\n            #print(parent1)\n            #print(parent2)\n            ## Make a few random muytations\n            #mutation_size = random.random()\n            #for random_change in range(math.floor(n*mutation_size)):\n            #    i, j = random.sample(range(n), 2)\n            #    parent[i], parent[j] = parent[j], parent[i]\n\n            #new_population.append(\" \".join(parent))\n            child1, child2 = order_crossover(parent1, parent2, random_position = random_position)\n            # 50% chance of mutation\n            if random.random() < mutation_rate:\n                mutate(child1)\n            if random.random() < mutation_rate:\n                mutate(child2)\n            new_population.append(child1)\n            new_population.append(child2)\n\n        # V2, adding more random search for the larger texts' exponentially increasing search spaces\n        #new_population += [\" \".join(random.sample(words, n)) for _ in range(population_size)]\n\n        \n        # Check for plateau, aggressively introduce more random search\n        # Environment can be hard sometimes. We need mutations.\n        #if last_best_edited > 3 and np.min(scores) >= last_best_score and last_mutation > 3:\n        if top_scores[0:3] == last_best_score:\n            print_sequence = \" \".join(top_sequences[0])\n            print(f\"Oh no, plateau at iteration {iteration}. Best sequence is {print_sequence}. Our population mutates.\")\n            ## Make a few random mutations\n            new_population2 = []\n            for child in new_population:\n                #child = child.split()\n                #random.shuffle(child)\n                #child = \" \".join(child)\n                child = mutate(child)\n                #child = mutate(child)\n                #child = mutate(child)\n                new_population2.append(child)\n            new_population = new_population2\n            last_mutation = 0\n\n            #new_population += [\" \".join(random.sample(words, n)) for _ in range(population_size*20)]\n\n        # Don't seem to be going anywhere\n        #if last_best_edited >= 20:\n        #    print(\"Finding Nothing, Breaking off\")\n        #    break\n\n        # Score the new population and update\n        # Initialize a set to track unique solutions \n        # This needs to be changed or commented out for dealing with word combinations\n        unique_solutions = set() \n        ## Add solutions to the set \n        for solution in new_population: \n            unique_solutions.add(\" \".join(solution)) \n        ## Convert the set back to a list \n        new_population = list(unique_solutions)\n        new_population = [solution.split() for solution in unique_solutions]\n        population = new_population\n        \n        \n        last_best_score = top_scores[0:3] #min(scores)\n        scores = scorer.get_perplexity([\" \".join(new_population[i]) for i in range(len(population))], batch_size=4)\n    \n    return best_sequence, best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T11:37:47.104579Z","iopub.execute_input":"2024-12-16T11:37:47.105054Z","iopub.status.idle":"2024-12-16T11:37:47.119802Z","shell.execute_reply.started":"2024-12-16T11:37:47.105024Z","shell.execute_reply":"2024-12-16T11:37:47.118978Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Let's Go","metadata":{}},{"cell_type":"markdown","source":"We choose sample 1 and a shuffle of sample 1 as starting points. The second parent should be very different from the first.","metadata":{}},{"cell_type":"code","source":"population_size = 200\ntop_k = 40\niterations = 30\nmutation_rate = 0.2\nrandom_position = True\n\n#for idx, text in enumerate(df[\"text\"].to_list()):\npopulation = []\nsanta1 = \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge\"\nsanta2 = \"family ornament scrooge chimney gingerbread advent reindeer fireplace mistletoe elf\"\npopulation.append(santa1.split())\npopulation.append(santa2.split())\nbest_sequence, best_score = genetic_search(\n        population, scorer, iterations=iterations, population_size=population_size, top_k=top_k,\n        mutation_rate = mutation_rate, random_position = random_position,\n    )\nprint_sequence = \" \".join(best_sequence)\nprint(f\"Sample {1}: Best perplexity={best_score:.2f}, Best permutation: {print_sequence}\")\ndf.loc[0, \"text\"] = print_sequence\ndf.loc[0,\"score\"] = best_score\n\ndf.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-12-16T11:37:47.120761Z","iopub.execute_input":"2024-12-16T11:37:47.121094Z","iopub.status.idle":"2024-12-16T11:43:50.695403Z","shell.execute_reply.started":"2024-12-16T11:37:47.121067Z","shell.execute_reply":"2024-12-16T11:43:50.694402Z"}},"outputs":[{"name":"stdout","text":"Starting \n  ------------------------------------------------------\n\nBest Scores: [3887.9021574548156, 7853.778244357167] on 0th iteration in 0.6811323165893555s\nBest Scores: [1430.2792730137226, 1577.0026226114692, 1608.1062123274673] on 1th iteration in 7.68685507774353s\nBest Scores: [975.3657218509629, 1223.3826929169447, 1242.6481670549958] on 2th iteration in 19.950644731521606s\nBest Scores: [864.1261369598841, 898.5490089196367, 927.0720162676876] on 3th iteration in 32.306334495544434s\nBest Scores: [571.1525003973912, 639.6604572352473, 672.9821121368258] on 4th iteration in 44.67410850524902s\nBest Scores: [571.1525003973912, 622.4067077254782, 627.2883040361144] on 5th iteration in 57.32476496696472s\nBest Scores: [538.6481096972173, 562.2976016279617, 571.1525003973912] on 6th iteration in 69.69499826431274s\nBest Scores: [538.6481096972173, 557.921766975976, 566.7077563764824] on 7th iteration in 81.84695410728455s\nBest Scores: [528.2297121579952, 557.921766975976, 562.2976016279617] on 8th iteration in 94.39845943450928s\nBest Scores: [484.7308313654974, 528.2297121579952, 555.7466361419574] on 9th iteration in 106.41323161125183s\nBest Scores: [504.04031250414465, 511.9777924614019, 520.0402695399388] on 10th iteration in 118.59777450561523s\nBest Scores: [504.04031250414465, 511.9777924614019, 520.0402695399388] on 11th iteration in 130.4092926979065s\nOh no, plateau at iteration 11. Best sequence is reindeer mistletoe elf gingerbread family scrooge chimney fireplace ornament advent. Our population mutates.\nBest Scores: [522.0756496064025, 530.2971447966088, 542.872779181642] on 12th iteration in 143.28706884384155s\nBest Scores: [522.0756496064025, 528.2297121579952, 530.2971447966088] on 13th iteration in 155.6021568775177s\nBest Scores: [467.98558773246197, 513.9816168847873, 528.2297121579952] on 14th iteration in 167.90208745002747s\nBest Scores: [511.9777924614019, 513.9816168847873, 515.9932840552193] on 15th iteration in 180.1445279121399s\nBest Scores: [511.9777924614019, 513.9816168847873, 515.9932840552193] on 16th iteration in 192.4984152317047s\nOh no, plateau at iteration 16. Best sequence is reindeer mistletoe elf gingerbread family ornament advent scrooge chimney fireplace. Our population mutates.\nBest Scores: [522.0756496064025, 528.2297121579952, 573.3879280792671] on 17th iteration in 205.31607699394226s\nBest Scores: [511.9777924614019, 515.9932840552193, 520.0402695399388] on 18th iteration in 217.4746537208557s\nBest Scores: [515.9932840552193, 520.0402695399388, 522.0756496064025] on 19th iteration in 229.81871128082275s\nBest Scores: [511.9777924614019, 515.9932840552193, 522.0756496064025] on 20th iteration in 242.30427265167236s\nBest Scores: [467.98558773246197, 494.29129035606036, 511.9777924614019] on 21th iteration in 254.50733613967896s\nBest Scores: [467.98558773246197, 494.29129035606036, 511.9777924614019] on 22th iteration in 266.4521977901459s\nOh no, plateau at iteration 22. Best sequence is reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament. Our population mutates.\nBest Scores: [515.9932840552193, 536.5481197318883, 542.872779181642] on 23th iteration in 279.39848709106445s\nBest Scores: [467.98558773246197, 504.04031250414465, 536.5481197318883] on 24th iteration in 291.69877314567566s\nBest Scores: [467.98558773246197, 504.04031250414465, 511.9777924614019] on 25th iteration in 304.2247009277344s\nBest Scores: [467.98558773246197, 511.9777924614019, 528.2297121579952] on 26th iteration in 316.28537702560425s\nBest Scores: [467.98558773246197, 511.9777924614019, 515.9932840552193] on 27th iteration in 328.2580852508545s\nBest Scores: [467.98558773246197, 504.04031250414465, 520.0402695399388] on 28th iteration in 340.0704448223114s\nBest Scores: [504.04031250414465, 511.9777924614019, 515.9932840552193] on 29th iteration in 351.6789655685425s\nSample 1: Best perplexity=467.99, Best permutation: reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\n","output_type":"stream"}],"execution_count":9}]}