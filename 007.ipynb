{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, you will learn optimization techniques on the subject of Santa-2024 competition.<br>\nsanta-2024のコンペを題材に、最適化手法を学んでいきます。<br>\n1. [Issues in Competition (コンペにおける課題)](https://www.kaggle.com/code/utm529fg/eng-tutolial-1-issues-in-competitions)\n2. [Greedy (貪欲法)](https://www.kaggle.com/code/utm529fg/eng-tutolial-2-greedy)\n3. BS: Beam Search (ビームサーチ) ← this notebook\n4. HC: Hill Climbing (山登り法)\n5. SA: Simulated Annealing (焼きなまし法)\n6. GA: Genetic Algorithm (遺伝的アルゴリズム)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 3-1. What's Beam Search Algrithm　ビームサーチとは\nIn the previous greedy tutorial, the idea was to decide on a sequence of 10 words, one word at a time, in order from the top to the bottom to maximize the score. In beam search, we do not settle on a single word, but keep, for example, the top three candidates and move on to the next word.<br>\nThe number of top candidates is called the beam width.\nThe algorithm is as follows.<br>\n**Step 1**. Decide the `beam_width`.<br>\n**Step 2**. Calculate the perplexity score for one word only.<br>\n**Step 3**. The top scoring `beam_width` words are tentative text candidates.<br>\n**Step 4**. Calculate the two-word perplexity score for each of the remaining candidate words as the second word.<br>\n**Step 5**. The top `beam_width` words with the best scores are selected as tentative text candidates. step6.<br>\n**Step 6**. Repeat steps 4-5, increasing the number of words one by one.<br>\nThe larger the beam_width, the more text is left as candidates, which increases the computation time, but the better the score is expected to be.<br>\n\n前回の貪欲法では、10個の単語の並びを頭から順にスコアが最大になるように1単語ずつ決めていくという考え方でした。\nビームサーチでは、1単語に決めず、例えば上位3つまで候補を残して、次の単語の検討に移ります。<br>\n上位何個まで残すかの数値を、ビーム幅と呼びます。<br>\n以下のようなアルゴリズムです。<br>\n**Step 1**. ビーム幅 `beam_width` を決める。<br>\n**Step 2**. 1単語のみで、perplexityスコアを計算する。<br>\n**Step 3**. スコアの良い上位 `beam_width` 個の単語を暫定のテキスト候補とする。<br>\n**Step 4**. 候補に残りの単語をそれぞれ2番目の単語とした時の、2単語でのperplexityスコアを計算する。<br>\n**Step 5**. スコアの良い上位 `beam_width` 個のテキストを暫定のテキスト候補とする。<br>\n**Step 6**. 4-5の手順を単語数の数だけ一つずつ増やしながら繰り返す。<br>\nビーム幅を大きくするほど、候補として残すテキストが増えるため、計算時間は増えますが、スコアは良くなることが期待できます。","metadata":{}},{"cell_type":"markdown","source":"# 3-2. Implementation 実装","metadata":{}},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        debug : bool, default=False\n            Print debugging information.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-17T10:42:17.774166Z","iopub.execute_input":"2024-12-17T10:42:17.774807Z","iopub.status.idle":"2024-12-17T10:42:17.797329Z","shell.execute_reply.started":"2024-12-17T10:42:17.774769Z","shell.execute_reply":"2024-12-17T10:42:17.796174Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/santa-2024/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T10:42:18.440599Z","iopub.execute_input":"2024-12-17T10:42:18.440976Z","iopub.status.idle":"2024-12-17T10:42:18.448134Z","shell.execute_reply.started":"2024-12-17T10:42:18.440944Z","shell.execute_reply":"2024-12-17T10:42:18.447257Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# initialize scorer\nscorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T10:42:18.882096Z","iopub.execute_input":"2024-12-17T10:42:18.883319Z","iopub.status.idle":"2024-12-17T10:45:17.271559Z","shell.execute_reply.started":"2024-12-17T10:42:18.883254Z","shell.execute_reply":"2024-12-17T10:45:17.270707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607e9db162d1475ba127ce0b24ef2710"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class BeamSearch():\n    def __init__(self, beam_width):\n        self.beam_width = beam_width\n\n    def _make_candidates_adding_word(self, intermediate_solution, words):\n        '''Make candidate text so that word duplication does not occur'''\n        candidates = []\n        not_used_words = words.copy()\n        for used_word in intermediate_solution.split():\n            not_used_words.remove(used_word)\n        \n        candidates += [(intermediate_solution + ' ' + word).lstrip() for word in not_used_words]\n        candidates += [(word + ' ' + intermediate_solution).rstrip() for word in not_used_words]\n\n        candidates = list(set(candidates)) # drop duplicate\n        \n        return candidates\n    \n    def solve(self, text):\n        words = text.split()\n        intermediate_solutions = ['']\n        eval_cnt = 0\n\n        while len(intermediate_solutions[0].split()) < len(words):\n            candidates = []\n            for intermediate_solution in intermediate_solutions:\n                candidates += self._make_candidates_adding_word(intermediate_solution, words)\n            scores = scorer.get_perplexity(candidates)\n            eval_cnt += len(scores)\n            top_idx = sorted(range(len(scores)), key=lambda i: scores[i])[:self.beam_width]\n            intermediate_solutions = [candidates[i] for i in top_idx]\n\n        best_score = min(scores)\n        best_solution = candidates[np.argmin(scores)]\n\n        return best_score, best_solution, eval_cnt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T10:45:17.273042Z","iopub.execute_input":"2024-12-17T10:45:17.273605Z","iopub.status.idle":"2024-12-17T10:45:17.282651Z","shell.execute_reply.started":"2024-12-17T10:45:17.273567Z","shell.execute_reply":"2024-12-17T10:45:17.281689Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"params = {\n    'beam_width': 3,\n}\noptimizer = BeamSearch(**params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T10:45:17.283588Z","iopub.execute_input":"2024-12-17T10:45:17.283836Z","iopub.status.idle":"2024-12-17T10:45:17.297394Z","shell.execute_reply.started":"2024-12-17T10:45:17.283794Z","shell.execute_reply":"2024-12-17T10:45:17.296670Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"sample_submission.loc[0][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:10:24.487233Z","iopub.execute_input":"2024-12-17T11:10:24.488170Z","iopub.status.idle":"2024-12-17T11:10:24.494331Z","shell.execute_reply.started":"2024-12-17T11:10:24.488119Z","shell.execute_reply":"2024-12-17T11:10:24.493373Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\nscores = []\nsolutions = []\neval_cnts = []\n# for text in tqdm(sample_submission['text']):\ntext = sample_submission.loc[0][\"text\"]\nscore, solution, eval_cnt = optimizer.solve(text)\nscores.append(score)\nsolutions.append(solution)\neval_cnts.append(eval_cnt)\n\nprint(scores)\nprint(solutions)\nprint(eval_cnts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:11:14.627733Z","iopub.execute_input":"2024-12-17T11:11:14.628537Z","iopub.status.idle":"2024-12-17T11:11:31.461442Z","shell.execute_reply.started":"2024-12-17T11:11:14.628491Z","shell.execute_reply":"2024-12-17T11:11:31.460325Z"}},"outputs":[{"name":"stdout","text":"[1050.510221115274]\n['ornament scrooge mistletoe reindeer elf gingerbread chimney fireplace advent family']\n[145]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"best_score = 1 << 60\nbest_blind = -1\nfor i in range(1000):\n    score = scorer.get_perplexity(\" \"*i + \"from and and as and have the in is it of not that the to we with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game give gifts gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\")\n    if best_score > score:\n        best_score = score\n        best_blind = i\n    print(f\"blind, score:{i}, {score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:21:31.252604Z","iopub.execute_input":"2024-12-17T11:21:31.252938Z","iopub.status.idle":"2024-12-17T11:24:46.271426Z","shell.execute_reply.started":"2024-12-17T11:21:31.252910Z","shell.execute_reply":"2024-12-17T11:24:46.270498Z"}},"outputs":[{"name":"stdout","text":"blind, score:0, 34.63697922255266\nblind, score:1, 40.022953418667065\nblind, score:2, 43.444504934395866\nblind, score:3, 38.71588573814723\nblind, score:4, 37.01518097066168\nblind, score:5, 39.634006510082365\nblind, score:6, 38.33964103892552\nblind, score:7, 40.022953418667065\nblind, score:8, 38.71588573814723\nblind, score:9, 41.29341833835342\nblind, score:10, 40.17959882934157\nblind, score:11, 40.73270164287187\nblind, score:12, 39.40245503911115\nblind, score:13, 40.81233544283598\nblind, score:14, 40.81233544283598\nblind, score:15, 40.73270164287187\nblind, score:16, 39.86691871057298\nblind, score:17, 41.2128458403547\nblind, score:18, 41.45503620813183\nblind, score:19, 41.45503620813183\nblind, score:20, 41.132430556936306\nblind, score:21, 42.27266353442275\nblind, score:22, 41.861853726676614\nblind, score:23, 41.943695056893915\nblind, score:24, 41.29341833835342\nblind, score:25, 42.190180314697045\nblind, score:26, 42.190180314697045\nblind, score:27, 41.861853726676614\nblind, score:28, 41.861853726676614\nblind, score:29, 41.943695056893915\nblind, score:30, 41.45503620813183\nblind, score:31, 41.29341833835342\nblind, score:32, 43.5294404008871\nblind, score:33, 40.25815129472595\nblind, score:34, 39.86691871057298\nblind, score:35, 40.41571724385996\nblind, score:36, 39.095822701298815\nblind, score:37, 40.1011996370117\nblind, score:38, 39.86691871057298\nblind, score:39, 40.25815129472595\nblind, score:40, 39.7114923241556\nblind, score:41, 40.41571724385996\nblind, score:42, 40.1011996370117\nblind, score:43, 40.1011996370117\nblind, score:44, 40.022953418667065\nblind, score:45, 40.892124929554\nblind, score:46, 40.49473132867618\nblind, score:47, 40.41571724385996\nblind, score:48, 40.1011996370117\nblind, score:49, 41.132430556936306\nblind, score:50, 40.65322322588273\nblind, score:51, 40.73270164287187\nblind, score:52, 40.65322322588273\nblind, score:53, 40.892124929554\nblind, score:54, 40.41571724385996\nblind, score:55, 40.73270164287187\nblind, score:56, 40.022953418667065\nblind, score:57, 40.892124929554\nblind, score:58, 40.73270164287187\nblind, score:59, 40.57389988868238\nblind, score:60, 40.33685733281881\nblind, score:61, 40.73270164287187\nblind, score:62, 41.861853726676614\nblind, score:63, 43.444504934395866\nblind, score:64, 39.479488162280596\nblind, score:65, 39.479488162280596\nblind, score:66, 39.556671887793456\nblind, score:67, 39.7114923241556\nblind, score:68, 39.7114923241556\nblind, score:69, 39.556671887793456\nblind, score:70, 39.479488162280596\nblind, score:71, 39.7114923241556\nblind, score:72, 39.7114923241556\nblind, score:73, 39.7114923241556\nblind, score:74, 39.479488162280596\nblind, score:75, 39.78912962559818\nblind, score:76, 40.022953418667065\nblind, score:77, 39.94485987582193\nblind, score:78, 39.78912962559818\nblind, score:79, 40.022953418667065\nblind, score:80, 40.022953418667065\nblind, score:81, 39.86691871057298\nblind, score:82, 39.94485987582193\nblind, score:83, 40.1011996370117\nblind, score:84, 40.1011996370117\nblind, score:85, 40.022953418667065\nblind, score:86, 39.86691871057298\nblind, score:87, 39.94485987582193\nblind, score:88, 39.94485987582193\nblind, score:89, 39.94485987582193\nblind, score:90, 39.7114923241556\nblind, score:91, 39.634006510082365\nblind, score:92, 40.17959882934157\nblind, score:93, 41.05217218133816\nblind, score:94, 42.27266353442275\nblind, score:95, 38.56494680401705\nblind, score:96, 38.56494680401705\nblind, score:97, 38.56494680401705\nblind, score:98, 38.94340253332466\nblind, score:99, 39.01953819342525\nblind, score:100, 38.79157659527073\nblind, score:101, 38.79157659527073\nblind, score:102, 39.01953819342525\nblind, score:103, 39.01953819342525\nblind, score:104, 38.94340253332466\nblind, score:105, 38.71588573814723\nblind, score:106, 39.01953819342525\nblind, score:107, 39.248839424943355\nblind, score:108, 39.172256347947744\nblind, score:109, 39.095822701298815\nblind, score:110, 39.32557222442699\nblind, score:111, 39.32557222442699\nblind, score:112, 39.248839424943355\nblind, score:113, 39.248839424943355\nblind, score:114, 39.479488162280596\nblind, score:115, 39.634006510082365\nblind, score:116, 39.479488162280596\nblind, score:117, 39.40245503911115\nblind, score:118, 39.32557222442699\nblind, score:119, 39.479488162280596\nblind, score:120, 39.556671887793456\nblind, score:121, 39.248839424943355\nblind, score:122, 39.172256347947744\nblind, score:123, 39.634006510082365\nblind, score:124, 40.57389988868238\nblind, score:125, 41.05217218133816\nblind, score:126, 37.89297069129353\nblind, score:127, 37.89297069129353\nblind, score:128, 37.8190332109868\nblind, score:129, 38.0412795852805\nblind, score:130, 38.19016894386643\nblind, score:131, 38.115651564714454\nblind, score:132, 38.0412795852805\nblind, score:133, 38.26483200699776\nblind, score:134, 38.26483200699776\nblind, score:135, 38.19016894386643\nblind, score:136, 37.9670527218579\nblind, score:137, 38.26483200699776\nblind, score:138, 38.48969815122384\nblind, score:139, 38.48969815122384\nblind, score:140, 38.26483200699776\nblind, score:141, 38.64034257045414\nblind, score:142, 38.71588573814723\nblind, score:143, 38.56494680401705\nblind, score:144, 38.56494680401705\nblind, score:145, 38.71588573814723\nblind, score:146, 38.79157659527073\nblind, score:147, 38.71588573814723\nblind, score:148, 38.56494680401705\nblind, score:149, 38.64034257045414\nblind, score:150, 38.71588573814723\nblind, score:151, 38.79157659527073\nblind, score:152, 38.26483200699776\nblind, score:153, 38.41459632502361\nblind, score:154, 38.79157659527073\nblind, score:155, 39.248839424943355\nblind, score:156, 39.556671887793456\nblind, score:157, 37.01518097066168\nblind, score:158, 37.01518097066168\nblind, score:159, 37.01518097066168\nblind, score:160, 37.160054292819765\nblind, score:161, 37.305494633128845\nblind, score:162, 37.160054292819765\nblind, score:163, 37.08754689283638\nblind, score:164, 37.305494633128845\nblind, score:165, 37.305494633128845\nblind, score:166, 37.305494633128845\nblind, score:167, 37.01518097066168\nblind, score:168, 37.232703447205715\nblind, score:169, 37.5980852538711\nblind, score:170, 37.45150421083523\nblind, score:171, 37.305494633128845\nblind, score:172, 37.67159077349981\nblind, score:173, 37.67159077349981\nblind, score:174, 37.67159077349981\nblind, score:175, 37.45150421083523\nblind, score:176, 37.74523999888851\nblind, score:177, 37.74523999888851\nblind, score:178, 37.74523999888851\nblind, score:179, 37.45150421083523\nblind, score:180, 37.67159077349981\nblind, score:181, 37.74523999888851\nblind, score:182, 37.74523999888851\nblind, score:183, 37.305494633128845\nblind, score:184, 37.305494633128845\nblind, score:185, 37.74523999888851\nblind, score:186, 38.0412795852805\nblind, score:187, 38.33964103892552\nblind, score:188, 36.08717359043372\nblind, score:189, 36.08717359043372\nblind, score:190, 36.016759615546256\nblind, score:191, 36.228414794400415\nblind, score:192, 36.299242562272255\nblind, score:193, 36.15772522700737\nblind, score:194, 36.15772522700737\nblind, score:195, 36.44131378072682\nblind, score:196, 36.299242562272255\nblind, score:197, 36.299242562272255\nblind, score:198, 36.08717359043372\nblind, score:199, 36.299242562272255\nblind, score:200, 36.51255777326842\nblind, score:201, 36.58394105020859\nblind, score:202, 36.44131378072682\nblind, score:203, 36.65546388385301\nblind, score:204, 36.65546388385301\nblind, score:205, 36.65546388385301\nblind, score:206, 36.51255777326842\nblind, score:207, 36.65546388385301\nblind, score:208, 36.65546388385301\nblind, score:209, 36.72712654703974\nblind, score:210, 36.65546388385301\nblind, score:211, 36.65546388385301\nblind, score:212, 36.72712654703974\nblind, score:213, 36.72712654703974\nblind, score:214, 36.299242562272255\nblind, score:215, 36.44131378072682\nblind, score:216, 36.65546388385301\nblind, score:217, 36.942956250241515\nblind, score:218, 37.37842812826559\nblind, score:219, 35.32013242426123\nblind, score:220, 35.32013242426123\nblind, score:221, 35.32013242426123\nblind, score:222, 35.45837151398557\nblind, score:223, 35.45837151398557\nblind, score:224, 35.38918446958933\nblind, score:225, 35.32013242426123\nblind, score:226, 35.59715165620933\nblind, score:227, 35.52769382137767\nblind, score:228, 35.52769382137767\nblind, score:229, 35.32013242426123\nblind, score:230, 35.59715165620933\nblind, score:231, 35.66674528344124\nblind, score:232, 35.66674528344124\nblind, score:233, 35.59715165620933\nblind, score:234, 35.806340977539655\nblind, score:235, 35.7364749685521\nblind, score:236, 35.7364749685521\nblind, score:237, 35.59715165620933\nblind, score:238, 35.7364749685521\nblind, score:239, 35.806340977539655\nblind, score:240, 35.806340977539655\nblind, score:241, 35.7364749685521\nblind, score:242, 35.806340977539655\nblind, score:243, 35.806340977539655\nblind, score:244, 35.806340977539655\nblind, score:245, 35.59715165620933\nblind, score:246, 35.59715165620933\nblind, score:247, 35.806340977539655\nblind, score:248, 36.08717359043372\nblind, score:249, 36.44131378072682\nblind, score:250, 34.56939489431364\nblind, score:251, 34.501942437892716\nblind, score:252, 34.501942437892716\nblind, score:253, 34.63697922255266\nblind, score:254, 34.63697922255266\nblind, score:255, 34.63697922255266\nblind, score:256, 34.501942437892716\nblind, score:257, 34.77254452624438\nblind, score:258, 34.70469568042362\nblind, score:259, 34.70469568042362\nblind, score:260, 34.501942437892716\nblind, score:261, 34.70469568042362\nblind, score:262, 34.84052601883783\nblind, score:263, 34.84052601883783\nblind, score:264, 34.70469568042362\nblind, score:265, 34.908640417532865\nblind, score:266, 34.908640417532865\nblind, score:267, 34.908640417532865\nblind, score:268, 34.70469568042362\nblind, score:269, 34.84052601883783\nblind, score:270, 34.908640417532865\nblind, score:271, 34.97688798216538\nblind, score:272, 34.77254452624438\nblind, score:273, 34.908640417532865\nblind, score:274, 34.908640417532865\nblind, score:275, 34.908640417532865\nblind, score:276, 34.70469568042362\nblind, score:277, 34.77254452624438\nblind, score:278, 34.908640417532865\nblind, score:279, 35.18243227767251\nblind, score:280, 35.38918446958933\nblind, score:281, 33.76859572409541\nblind, score:282, 33.83461446305701\nblind, score:283, 33.76859572409541\nblind, score:284, 33.90076227087091\nblind, score:285, 33.83461446305701\nblind, score:286, 33.90076227087091\nblind, score:287, 33.702705802144536\nblind, score:288, 33.90076227087091\nblind, score:289, 33.90076227087091\nblind, score:290, 33.90076227087091\nblind, score:291, 33.702705802144536\nblind, score:292, 33.90076227087091\nblind, score:293, 33.96703939987107\nblind, score:294, 34.03344610288473\nblind, score:295, 33.90076227087091\nblind, score:296, 34.09998263323346\nblind, score:297, 34.09998263323346\nblind, score:298, 34.16664924473404\nblind, score:299, 33.90076227087091\nblind, score:300, 34.03344610288473\nblind, score:301, 34.03344610288473\nblind, score:302, 34.16664924473404\nblind, score:303, 33.96703939987107\nblind, score:304, 34.03344610288473\nblind, score:305, 34.09998263323346\nblind, score:306, 34.09998263323346\nblind, score:307, 33.90076227087091\nblind, score:308, 33.96703939987107\nblind, score:309, 34.03344610288473\nblind, score:310, 34.300373728940066\nblind, score:311, 34.4346215959791\nblind, score:312, 32.92198346312245\nblind, score:313, 32.9863470466753\nblind, score:314, 32.9863470466753\nblind, score:315, 33.11545195869231\nblind, score:316, 33.05083646319603\nblind, score:317, 33.05083646319603\nblind, score:318, 32.92198346312245\nblind, score:319, 33.1801937796528\nblind, score:320, 33.11545195869231\nblind, score:321, 33.1801937796528\nblind, score:322, 32.92198346312245\nblind, score:323, 33.11545195869231\nblind, score:324, 33.11545195869231\nblind, score:325, 33.245062173048\nblind, score:326, 33.11545195869231\nblind, score:327, 33.245062173048\nblind, score:328, 33.245062173048\nblind, score:329, 33.375179667439795\nblind, score:330, 33.1801937796528\nblind, score:331, 33.1801937796528\nblind, score:332, 33.245062173048\nblind, score:333, 33.31005738633129\nblind, score:334, 33.1801937796528\nblind, score:335, 33.245062173048\nblind, score:336, 33.245062173048\nblind, score:337, 33.31005738633129\nblind, score:338, 33.11545195869231\nblind, score:339, 33.1801937796528\nblind, score:340, 33.245062173048\nblind, score:341, 33.50580642730562\nblind, score:342, 33.57131140436464\nblind, score:343, 32.159346537604755\nblind, score:344, 32.28521466114651\nblind, score:345, 32.22221914035047\nblind, score:346, 32.28521466114651\nblind, score:347, 32.28521466114651\nblind, score:348, 32.28521466114651\nblind, score:349, 32.159346537604755\nblind, score:350, 32.348333340301785\nblind, score:351, 32.28521466114651\nblind, score:352, 32.411575418595035\nblind, score:353, 32.159346537604755\nblind, score:354, 32.348333340301785\nblind, score:355, 32.348333340301785\nblind, score:356, 32.411575418595035\nblind, score:357, 32.28521466114651\nblind, score:358, 32.47494113727571\nblind, score:359, 32.411575418595035\nblind, score:360, 32.47494113727571\nblind, score:361, 32.348333340301785\nblind, score:362, 32.47494113727571\nblind, score:363, 32.47494113727571\nblind, score:364, 32.53843073806492\nblind, score:365, 32.411575418595035\nblind, score:366, 32.411575418595035\nblind, score:367, 32.411575418595035\nblind, score:368, 32.53843073806492\nblind, score:369, 32.348333340301785\nblind, score:370, 32.411575418595035\nblind, score:371, 32.411575418595035\nblind, score:372, 32.60204446315635\nblind, score:373, 32.79363281328884\nblind, score:374, 31.53732846903916\nblind, score:375, 31.53732846903916\nblind, score:376, 31.475792237910618\nblind, score:377, 31.598985005566618\nblind, score:378, 31.53732846903916\nblind, score:379, 31.598985005566618\nblind, score:380, 31.41437607743883\nblind, score:381, 31.660762082694095\nblind, score:382, 31.660762082694095\nblind, score:383, 31.660762082694095\nblind, score:384, 31.41437607743883\nblind, score:385, 31.598985005566618\nblind, score:386, 31.598985005566618\nblind, score:387, 31.660762082694095\nblind, score:388, 31.598985005566618\nblind, score:389, 31.72265993608251\nblind, score:390, 31.660762082694095\nblind, score:391, 31.72265993608251\nblind, score:392, 31.53732846903916\nblind, score:393, 31.660762082694095\nblind, score:394, 31.660762082694095\nblind, score:395, 31.72265993608251\nblind, score:396, 31.53732846903916\nblind, score:397, 31.660762082694095\nblind, score:398, 31.660762082694095\nblind, score:399, 31.660762082694095\nblind, score:400, 31.598985005566618\nblind, score:401, 31.660762082694095\nblind, score:402, 31.660762082694095\nblind, score:403, 31.784678801853513\nblind, score:404, 31.9714638416083\nblind, score:405, 30.74665629604239\nblind, score:406, 30.866995306309306\nblind, score:407, 30.74665629604239\nblind, score:408, 30.866995306309306\nblind, score:409, 30.806767041912167\nblind, score:410, 30.866995306309306\nblind, score:411, 30.68666283939559\nblind, score:412, 30.866995306309306\nblind, score:413, 30.866995306309306\nblind, score:414, 30.927341318986464\nblind, score:415, 30.68666283939559\nblind, score:416, 30.927341318986464\nblind, score:417, 30.866995306309306\nblind, score:418, 30.927341318986464\nblind, score:419, 30.806767041912167\nblind, score:420, 30.987805310145493\nblind, score:421, 30.927341318986464\nblind, score:422, 30.987805310145493\nblind, score:423, 30.806767041912167\nblind, score:424, 30.987805310145493\nblind, score:425, 30.987805310145493\nblind, score:426, 30.987805310145493\nblind, score:427, 30.866995306309306\nblind, score:428, 30.927341318986464\nblind, score:429, 30.927341318986464\nblind, score:430, 30.987805310145493\nblind, score:431, 30.806767041912167\nblind, score:432, 30.927341318986464\nblind, score:433, 30.927341318986464\nblind, score:434, 31.048387510438282\nblind, score:435, 31.230845679407448\nblind, score:436, 30.093129052017023\nblind, score:437, 30.151962130172098\nblind, score:438, 30.093129052017023\nblind, score:439, 30.151962130172098\nblind, score:440, 30.151962130172098\nblind, score:441, 30.151962130172098\nblind, score:442, 30.034410770075546\nblind, score:443, 30.151962130172098\nblind, score:444, 30.151962130172098\nblind, score:445, 30.21091022897123\nblind, score:446, 30.034410770075546\nblind, score:447, 30.151962130172098\nblind, score:448, 30.151962130172098\nblind, score:449, 30.21091022897123\nblind, score:450, 30.151962130172098\nblind, score:451, 30.21091022897123\nblind, score:452, 30.21091022897123\nblind, score:453, 30.26997357328364\nblind, score:454, 30.151962130172098\nblind, score:455, 30.21091022897123\nblind, score:456, 30.21091022897123\nblind, score:457, 30.26997357328364\nblind, score:458, 30.151962130172098\nblind, score:459, 30.151962130172098\nblind, score:460, 30.21091022897123\nblind, score:461, 30.26997357328364\nblind, score:462, 30.151962130172098\nblind, score:463, 30.21091022897123\nblind, score:464, 30.21091022897123\nblind, score:465, 30.329152388418176\nblind, score:466, 30.447857334592324\nblind, score:467, 29.33866439394716\nblind, score:468, 29.453492679719986\nblind, score:469, 29.33866439394716\nblind, score:470, 29.453492679719986\nblind, score:471, 29.396022468352484\nblind, score:472, 29.453492679719986\nblind, score:473, 29.281418237700255\nblind, score:474, 29.453492679719986\nblind, score:475, 29.511075247281195\nblind, score:476, 29.511075247281195\nblind, score:477, 29.281418237700255\nblind, score:478, 29.453492679719986\nblind, score:479, 29.453492679719986\nblind, score:480, 29.511075247281195\nblind, score:481, 29.396022468352484\nblind, score:482, 29.511075247281195\nblind, score:483, 29.511075247281195\nblind, score:484, 29.626578330054702\nblind, score:485, 29.453492679719986\nblind, score:486, 29.56877039069624\nblind, score:487, 29.56877039069624\nblind, score:488, 29.626578330054702\nblind, score:489, 29.453492679719986\nblind, score:490, 29.511075247281195\nblind, score:491, 29.511075247281195\nblind, score:492, 29.56877039069624\nblind, score:493, 29.453492679719986\nblind, score:494, 29.511075247281195\nblind, score:495, 29.511075247281195\nblind, score:496, 29.626578330054702\nblind, score:497, 29.74253347911242\nblind, score:498, 28.65903496526287\nblind, score:499, 28.771203256660744\nblind, score:500, 28.715064341305684\nblind, score:501, 28.771203256660744\nblind, score:502, 28.715064341305684\nblind, score:503, 28.771203256660744\nblind, score:504, 28.60311491479713\nblind, score:505, 28.771203256660744\nblind, score:506, 28.771203256660744\nblind, score:507, 28.827451925481085\nblind, score:508, 28.60311491479713\nblind, score:509, 28.771203256660744\nblind, score:510, 28.771203256660744\nblind, score:511, 28.827451925481085\nblind, score:512, 28.715064341305684\nblind, score:513, 28.883810562338418\nblind, score:514, 28.827451925481085\nblind, score:515, 28.940279382223952\nblind, score:516, 28.771203256660744\nblind, score:517, 28.827451925481085\nblind, score:518, 28.827451925481085\nblind, score:519, 28.940279382223952\nblind, score:520, 28.771203256660744\nblind, score:521, 28.827451925481085\nblind, score:522, 28.827451925481085\nblind, score:523, 28.883810562338418\nblind, score:524, 28.771203256660744\nblind, score:525, 28.827451925481085\nblind, score:526, 28.827451925481085\nblind, score:527, 28.940279382223952\nblind, score:528, 29.053548433146837\nblind, score:529, 27.995149135337257\nblind, score:530, 28.1047190510635\nblind, score:531, 28.049880592281973\nblind, score:532, 28.1047190510635\nblind, score:533, 28.1047190510635\nblind, score:534, 28.1047190510635\nblind, score:535, 27.995149135337257\nblind, score:536, 28.1047190510635\nblind, score:537, 28.1047190510635\nblind, score:538, 28.15966472087403\nblind, score:539, 27.995149135337257\nblind, score:540, 28.15966472087403\nblind, score:541, 28.1047190510635\nblind, score:542, 28.15966472087403\nblind, score:543, 28.1047190510635\nblind, score:544, 28.214717811314713\nblind, score:545, 28.1047190510635\nblind, score:546, 28.214717811314713\nblind, score:547, 28.1047190510635\nblind, score:548, 28.214717811314713\nblind, score:549, 28.15966472087403\nblind, score:550, 28.214717811314713\nblind, score:551, 28.1047190510635\nblind, score:552, 28.15966472087403\nblind, score:553, 28.214717811314713\nblind, score:554, 28.214717811314713\nblind, score:555, 28.1047190510635\nblind, score:556, 28.15966472087403\nblind, score:557, 28.15966472087403\nblind, score:558, 28.2698785323965\nblind, score:559, 28.380523708580828\nblind, score:560, 27.40010580820802\nblind, score:561, 27.40010580820802\nblind, score:562, 27.40010580820802\nblind, score:563, 27.4536739354601\nblind, score:564, 27.40010580820802\nblind, score:565, 27.40010580820802\nblind, score:566, 27.346642204097876\nblind, score:567, 27.4536739354601\nblind, score:568, 27.4536739354601\nblind, score:569, 27.507346790200362\nblind, score:570, 27.346642204097876\nblind, score:571, 27.4536739354601\nblind, score:572, 27.4536739354601\nblind, score:573, 27.507346790200362\nblind, score:574, 27.346642204097876\nblind, score:575, 27.507346790200362\nblind, score:576, 27.4536739354601\nblind, score:577, 27.507346790200362\nblind, score:578, 27.4536739354601\nblind, score:579, 27.507346790200362\nblind, score:580, 27.507346790200362\nblind, score:581, 27.561124577174567\nblind, score:582, 27.4536739354601\nblind, score:583, 27.4536739354601\nblind, score:584, 27.4536739354601\nblind, score:585, 27.507346790200362\nblind, score:586, 27.4536739354601\nblind, score:587, 27.507346790200362\nblind, score:588, 27.4536739354601\nblind, score:589, 27.72308958496738\nblind, score:590, 27.83159468972004\nblind, score:591, 26.817710263716034\nblind, score:592, 26.87013978811538\nblind, score:593, 26.817710263716034\nblind, score:594, 26.87013978811538\nblind, score:595, 26.817710263716034\nblind, score:596, 26.817710263716034\nblind, score:597, 26.713157919740368\nblind, score:598, 26.87013978811538\nblind, score:599, 26.87013978811538\nblind, score:600, 26.87013978811538\nblind, score:601, 26.76538304079522\nblind, score:602, 26.87013978811538\nblind, score:603, 26.87013978811538\nblind, score:604, 26.87013978811538\nblind, score:605, 26.817710263716034\nblind, score:606, 26.87013978811538\nblind, score:607, 26.87013978811538\nblind, score:608, 26.922671813996086\nblind, score:609, 26.817710263716034\nblind, score:610, 26.87013978811538\nblind, score:611, 26.87013978811538\nblind, score:612, 26.87013978811538\nblind, score:613, 26.87013978811538\nblind, score:614, 26.87013978811538\nblind, score:615, 26.87013978811538\nblind, score:616, 26.87013978811538\nblind, score:617, 26.87013978811538\nblind, score:618, 26.87013978811538\nblind, score:619, 26.87013978811538\nblind, score:620, 27.080884906424416\nblind, score:621, 27.240027749911206\nblind, score:622, 26.196478674560588\nblind, score:623, 26.24769367033534\nblind, score:624, 26.196478674560588\nblind, score:625, 26.24769367033534\nblind, score:626, 26.196478674560588\nblind, score:627, 26.24769367033534\nblind, score:628, 26.14536361045317\nblind, score:629, 26.196478674560588\nblind, score:630, 26.24769367033534\nblind, score:631, 26.2990087931472\nblind, score:632, 26.14536361045317\nblind, score:633, 26.24769367033534\nblind, score:634, 26.196478674560588\nblind, score:635, 26.2990087931472\nblind, score:636, 26.14536361045317\nblind, score:637, 26.2990087931472\nblind, score:638, 26.24769367033534\nblind, score:639, 26.35042423874788\nblind, score:640, 26.196478674560588\nblind, score:641, 26.2990087931472\nblind, score:642, 26.2990087931472\nblind, score:643, 26.2990087931472\nblind, score:644, 26.196478674560588\nblind, score:645, 26.24769367033534\nblind, score:646, 26.24769367033534\nblind, score:647, 26.2990087931472\nblind, score:648, 26.196478674560588\nblind, score:649, 26.2990087931472\nblind, score:650, 26.24769367033534\nblind, score:651, 26.453556883236853\nblind, score:652, 26.557093177483523\nblind, score:653, 25.63966650134361\nblind, score:654, 25.689792910615846\nblind, score:655, 25.63966650134361\nblind, score:656, 25.689792910615846\nblind, score:657, 25.63966650134361\nblind, score:658, 25.689792910615846\nblind, score:659, 25.58963789966816\nblind, score:660, 25.689792910615846\nblind, score:661, 25.689792910615846\nblind, score:662, 25.740017318702005\nblind, score:663, 25.58963789966816\nblind, score:664, 25.689792910615846\nblind, score:665, 25.63966650134361\nblind, score:666, 25.740017318702005\nblind, score:667, 25.63966650134361\nblind, score:668, 25.740017318702005\nblind, score:669, 25.689792910615846\nblind, score:670, 25.790339917193062\nblind, score:671, 25.689792910615846\nblind, score:672, 25.740017318702005\nblind, score:673, 25.689792910615846\nblind, score:674, 25.790339917193062\nblind, score:675, 25.689792910615846\nblind, score:676, 25.740017318702005\nblind, score:677, 25.740017318702005\nblind, score:678, 25.740017318702005\nblind, score:679, 25.689792910615846\nblind, score:680, 25.689792910615846\nblind, score:681, 25.740017318702005\nblind, score:682, 25.891280453627324\nblind, score:683, 25.992616060150944\nblind, score:684, 25.094689498803355\nblind, score:685, 25.1437504597293\nblind, score:686, 25.094689498803355\nblind, score:687, 25.1437504597293\nblind, score:688, 25.094689498803355\nblind, score:689, 25.1437504597293\nblind, score:690, 24.9968545761854\nblind, score:691, 25.1437504597293\nblind, score:692, 25.192907336481866\nblind, score:693, 25.192907336481866\nblind, score:694, 25.045724266551254\nblind, score:695, 25.1437504597293\nblind, score:696, 25.1437504597293\nblind, score:697, 25.1437504597293\nblind, score:698, 25.094689498803355\nblind, score:699, 25.1437504597293\nblind, score:700, 25.1437504597293\nblind, score:701, 25.24216031657971\nblind, score:702, 25.1437504597293\nblind, score:703, 25.192907336481866\nblind, score:704, 25.094689498803355\nblind, score:705, 25.192907336481866\nblind, score:706, 25.1437504597293\nblind, score:707, 25.094689498803355\nblind, score:708, 25.192907336481866\nblind, score:709, 25.192907336481866\nblind, score:710, 25.094689498803355\nblind, score:711, 25.1437504597293\nblind, score:712, 25.1437504597293\nblind, score:713, 25.2915095879081\nblind, score:714, 25.39049775763493\nblind, score:715, 24.561296107667808\nblind, score:716, 24.561296107667808\nblind, score:717, 24.561296107667808\nblind, score:718, 24.561296107667808\nblind, score:719, 24.561296107667808\nblind, score:720, 24.561296107667808\nblind, score:721, 24.513371642677637\nblind, score:722, 24.561296107667808\nblind, score:723, 24.561296107667808\nblind, score:724, 24.561296107667808\nblind, score:725, 24.513371642677637\nblind, score:726, 24.561296107667808\nblind, score:727, 24.561296107667808\nblind, score:728, 24.561296107667808\nblind, score:729, 24.561296107667808\nblind, score:730, 24.609314266596865\nblind, score:731, 24.561296107667808\nblind, score:732, 24.609314266596865\nblind, score:733, 24.561296107667808\nblind, score:734, 24.561296107667808\nblind, score:735, 24.561296107667808\nblind, score:736, 24.609314266596865\nblind, score:737, 24.561296107667808\nblind, score:738, 24.561296107667808\nblind, score:739, 24.561296107667808\nblind, score:740, 24.609314266596865\nblind, score:741, 24.561296107667808\nblind, score:742, 24.561296107667808\nblind, score:743, 24.561296107667808\nblind, score:744, 24.70563239932894\nblind, score:745, 24.80232751057381\nblind, score:746, 23.992334296139543\nblind, score:747, 24.039240115615065\nblind, score:748, 23.992334296139543\nblind, score:749, 24.039240115615065\nblind, score:750, 23.992334296139543\nblind, score:751, 23.992334296139543\nblind, score:752, 23.94552000018515\nblind, score:753, 24.039240115615065\nblind, score:754, 24.039240115615065\nblind, score:755, 24.039240115615065\nblind, score:756, 23.898797049169467\nblind, score:757, 24.039240115615065\nblind, score:758, 23.992334296139543\nblind, score:759, 24.039240115615065\nblind, score:760, 23.94552000018515\nblind, score:761, 24.039240115615065\nblind, score:762, 23.992334296139543\nblind, score:763, 24.086237637543274\nblind, score:764, 23.992334296139543\nblind, score:765, 24.039240115615065\nblind, score:766, 23.992334296139543\nblind, score:767, 24.086237637543274\nblind, score:768, 23.992334296139543\nblind, score:769, 23.992334296139543\nblind, score:770, 24.039240115615065\nblind, score:771, 24.039240115615065\nblind, score:772, 23.94552000018515\nblind, score:773, 23.992334296139543\nblind, score:774, 23.992334296139543\nblind, score:775, 24.22778221261098\nblind, score:776, 24.275148340672132\nblind, score:777, 23.52828054362271\nblind, score:778, 23.52828054362271\nblind, score:779, 23.52828054362271\nblind, score:780, 23.52828054362271\nblind, score:781, 23.52828054362271\nblind, score:782, 23.52828054362271\nblind, score:783, 23.482371718117374\nblind, score:784, 23.52828054362271\nblind, score:785, 23.52828054362271\nblind, score:786, 23.52828054362271\nblind, score:787, 23.482371718117374\nblind, score:788, 23.52828054362271\nblind, score:789, 23.52828054362271\nblind, score:790, 23.52828054362271\nblind, score:791, 23.52828054362271\nblind, score:792, 23.52828054362271\nblind, score:793, 23.52828054362271\nblind, score:794, 23.574279122424027\nblind, score:795, 23.482371718117374\nblind, score:796, 23.52828054362271\nblind, score:797, 23.52828054362271\nblind, score:798, 23.574279122424027\nblind, score:799, 23.52828054362271\nblind, score:800, 23.482371718117374\nblind, score:801, 23.52828054362271\nblind, score:802, 23.52828054362271\nblind, score:803, 23.52828054362271\nblind, score:804, 23.52828054362271\nblind, score:805, 23.52828054362271\nblind, score:806, 23.75917448515314\nblind, score:807, 23.898797049169467\nblind, score:808, 23.073202403172917\nblind, score:809, 23.073202403172917\nblind, score:810, 23.028181534732802\nblind, score:811, 23.118311288923124\nblind, score:812, 23.073202403172917\nblind, score:813, 23.118311288923124\nblind, score:814, 22.98324851186175\nblind, score:815, 23.073202403172917\nblind, score:816, 23.118311288923124\nblind, score:817, 23.118311288923124\nblind, score:818, 22.98324851186175\nblind, score:819, 23.118311288923124\nblind, score:820, 23.028181534732802\nblind, score:821, 23.118311288923124\nblind, score:822, 23.028181534732802\nblind, score:823, 23.073202403172917\nblind, score:824, 23.073202403172917\nblind, score:825, 23.118311288923124\nblind, score:826, 23.028181534732802\nblind, score:827, 23.073202403172917\nblind, score:828, 23.073202403172917\nblind, score:829, 23.118311288923124\nblind, score:830, 23.073202403172917\nblind, score:831, 23.028181534732802\nblind, score:832, 23.073202403172917\nblind, score:833, 23.073202403172917\nblind, score:834, 23.028181534732802\nblind, score:835, 23.073202403172917\nblind, score:836, 23.073202403172917\nblind, score:837, 23.299630451611073\nblind, score:838, 23.345182011801924\nblind, score:839, 22.626926270737744\nblind, score:840, 22.67116267165818\nblind, score:841, 22.626926270737744\nblind, score:842, 22.67116267165818\nblind, score:843, 22.626926270737744\nblind, score:844, 22.67116267165818\nblind, score:845, 22.538712245181248\nblind, score:846, 22.626926270737744\nblind, score:847, 22.67116267165818\nblind, score:848, 22.67116267165818\nblind, score:849, 22.582776184718522\nblind, score:850, 22.626926270737744\nblind, score:851, 22.626926270737744\nblind, score:852, 22.67116267165818\nblind, score:853, 22.582776184718522\nblind, score:854, 22.626926270737744\nblind, score:855, 22.626926270737744\nblind, score:856, 22.67116267165818\nblind, score:857, 22.626926270737744\nblind, score:858, 22.67116267165818\nblind, score:859, 22.626926270737744\nblind, score:860, 22.715485556228362\nblind, score:861, 22.582776184718522\nblind, score:862, 22.626926270737744\nblind, score:863, 22.67116267165818\nblind, score:864, 22.67116267165818\nblind, score:865, 22.626926270737744\nblind, score:866, 22.67116267165818\nblind, score:867, 22.626926270737744\nblind, score:868, 22.80439145296227\nblind, score:869, 22.84897480427519\nblind, score:870, 22.145985754016134\nblind, score:871, 22.232662691545976\nblind, score:872, 22.145985754016134\nblind, score:873, 22.189281900071105\nblind, score:874, 22.145985754016134\nblind, score:875, 22.189281900071105\nblind, score:876, 22.10277408821932\nblind, score:877, 22.189281900071105\nblind, score:878, 22.189281900071105\nblind, score:879, 22.232662691545976\nblind, score:880, 22.10277408821932\nblind, score:881, 22.189281900071105\nblind, score:882, 22.145985754016134\nblind, score:883, 22.189281900071105\nblind, score:884, 22.10277408821932\nblind, score:885, 22.189281900071105\nblind, score:886, 22.145985754016134\nblind, score:887, 22.232662691545976\nblind, score:888, 22.10277408821932\nblind, score:889, 22.189281900071105\nblind, score:890, 22.145985754016134\nblind, score:891, 22.189281900071105\nblind, score:892, 22.145985754016134\nblind, score:893, 22.145985754016134\nblind, score:894, 22.189281900071105\nblind, score:895, 22.189281900071105\nblind, score:896, 22.145985754016134\nblind, score:897, 22.189281900071105\nblind, score:898, 22.189281900071105\nblind, score:899, 22.319678873017494\nblind, score:900, 22.319678873017494\nblind, score:901, 21.717643615002626\nblind, score:902, 21.717643615002626\nblind, score:903, 21.675267738479917\nblind, score:904, 21.717643615002626\nblind, score:905, 21.675267738479917\nblind, score:906, 21.717643615002626\nblind, score:907, 21.632974546568068\nblind, score:908, 21.717643615002626\nblind, score:909, 21.760102337787384\nblind, score:910, 21.760102337787384\nblind, score:911, 21.632974546568068\nblind, score:912, 21.717643615002626\nblind, score:913, 21.675267738479917\nblind, score:914, 21.675267738479917\nblind, score:915, 21.675267738479917\nblind, score:916, 21.717643615002626\nblind, score:917, 21.675267738479917\nblind, score:918, 21.760102337787384\nblind, score:919, 21.675267738479917\nblind, score:920, 21.717643615002626\nblind, score:921, 21.675267738479917\nblind, score:922, 21.717643615002626\nblind, score:923, 21.675267738479917\nblind, score:924, 21.675267738479917\nblind, score:925, 21.717643615002626\nblind, score:926, 21.760102337787384\nblind, score:927, 21.675267738479917\nblind, score:928, 21.717643615002626\nblind, score:929, 21.675267738479917\nblind, score:930, 21.887977204970085\nblind, score:931, 21.887977204970085\nblind, score:932, 21.256030110048442\nblind, score:933, 21.297586362924953\nblind, score:934, 21.256030110048442\nblind, score:935, 21.297586362924953\nblind, score:936, 21.256030110048442\nblind, score:937, 21.256030110048442\nblind, score:938, 21.256030110048442\nblind, score:939, 21.297586362924953\nblind, score:940, 21.297586362924953\nblind, score:941, 21.297586362924953\nblind, score:942, 21.214554942517648\nblind, score:943, 21.256030110048442\nblind, score:944, 21.256030110048442\nblind, score:945, 21.256030110048442\nblind, score:946, 21.256030110048442\nblind, score:947, 21.256030110048442\nblind, score:948, 21.256030110048442\nblind, score:949, 21.256030110048442\nblind, score:950, 21.256030110048442\nblind, score:951, 21.256030110048442\nblind, score:952, 21.256030110048442\nblind, score:953, 21.256030110048442\nblind, score:954, 21.214554942517648\nblind, score:955, 21.214554942517648\nblind, score:956, 21.256030110048442\nblind, score:957, 21.256030110048442\nblind, score:958, 21.256030110048442\nblind, score:959, 21.256030110048442\nblind, score:960, 21.214554942517648\nblind, score:961, 21.42274322042474\nblind, score:962, 21.42274322042474\nblind, score:963, 20.84490127137742\nblind, score:964, 20.8856537535641\nblind, score:965, 20.84490127137742\nblind, score:966, 20.84490127137742\nblind, score:967, 20.84490127137742\nblind, score:968, 20.84490127137742\nblind, score:969, 20.763634702888442\nblind, score:970, 20.8856537535641\nblind, score:971, 20.92648590822237\nblind, score:972, 20.8856537535641\nblind, score:973, 20.8042283062039\nblind, score:974, 20.8856537535641\nblind, score:975, 20.84490127137742\nblind, score:976, 20.84490127137742\nblind, score:977, 20.84490127137742\nblind, score:978, 20.8856537535641\nblind, score:979, 20.84490127137742\nblind, score:980, 20.8856537535641\nblind, score:981, 20.84490127137742\nblind, score:982, 20.8856537535641\nblind, score:983, 20.84490127137742\nblind, score:984, 20.8856537535641\nblind, score:985, 20.8042283062039\nblind, score:986, 20.763634702888442\nblind, score:987, 20.8856537535641\nblind, score:988, 20.8042283062039\nblind, score:989, 20.8042283062039\nblind, score:990, 20.8042283062039\nblind, score:991, 20.763634702888442\nblind, score:992, 20.8856537535641\nblind, score:993, 20.92648590822237\nblind, score:994, 20.44172438427552\nblind, score:995, 20.4816886421048\nblind, score:996, 20.44172438427552\nblind, score:997, 20.44172438427552\nblind, score:998, 20.44172438427552\nblind, score:999, 20.44172438427552\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(best_blind, best_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:24:46.272741Z","iopub.execute_input":"2024-12-17T11:24:46.273004Z","iopub.status.idle":"2024-12-17T11:24:46.277878Z","shell.execute_reply.started":"2024-12-17T11:24:46.272979Z","shell.execute_reply":"2024-12-17T11:24:46.276642Z"}},"outputs":[{"name":"stdout","text":"994 20.44172438427552\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# 3-3. Consideration 考察\nIncreasing the beam width will increase the search area and is expected to improve scores. You may want to experiment with different beam widths for different word counts.<br>\nAnother method, as with the greedy method, is to increase or change the pattern of words to be placed.\nIn the example implementation, an example of placement on the left is shown in the commented-out section.<br>\n\nビーム幅を大きくすると、さらに探索範囲が広がりスコアの改善が期待されます。単語数に合わせてビーム幅を変えて実験してみるのも良いでしょう。<br>\nまた、貪欲法の時と同じく、配置する単語の位置パターンを増やしたり変えてみる方法もあります。実装例では左に配置する例をコメントアウトで記載しています。","metadata":{}},{"cell_type":"markdown","source":"# 3-4. Submission 提出\nFinally, let's submit the solutions.<br>\n最後に、求めた解を提出してみましょう。","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:09:06.926560Z","iopub.status.idle":"2024-12-17T11:09:06.927056Z","shell.execute_reply.started":"2024-12-17T11:09:06.926799Z","shell.execute_reply":"2024-12-17T11:09:06.926824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}