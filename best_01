{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":10273909,"sourceType":"datasetVersion","datasetId":6356998},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"このノートブックは以下のノートブックを参考にして作成されました。\n\nこの場を借りてノートブックを共有してくださったことに感謝したいと思います。\n\nhttps://www.kaggle.com/code/utm529fg/eng-tutorial-3-beam-search\n\nまた、以下のノートブックを利用してid4以外の答えを作成しています。ありがとうございます。\n\nhttps://www.kaggle.com/code/jazivxt/diminutive-effort-tpu","metadata":{}},{"cell_type":"markdown","source":"This notebook was created with reference to the following notebook:\n\nhttps://www.kaggle.com/code/utm529fg/eng-tutorial-3-beam-search\n\nI would like to take this opportunity to express my gratitude to the author for sharing their notebook.\n\nAdditionally, I used the following notebook to generate answers other than id4. Thank you very much.\n\nhttps://www.kaggle.com/code/jazivxt/diminutive-effort-tpu","metadata":{}},{"cell_type":"markdown","source":"このノートブックはGPU環境下で実行に3時間程度かかります。実行される際は注意してください。\n\n元ノートブックからの変更点として、ビーム幅を可変にしています。\n\nこれにより、重要な局面で多くの情報を残すことができます。","metadata":{}},{"cell_type":"markdown","source":"This notebook takes approximately 3 hours to run in a GPU environment. Please be cautious when executing it.\n\nAs a modification from the original notebook, the beam width has been made adjustable.\n\nThis allows for retaining more information during critical stages.","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"print(\"Yes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T01:54:05.526568Z","iopub.execute_input":"2024-12-23T01:54:05.526887Z","iopub.status.idle":"2024-12-23T01:54:05.531845Z","shell.execute_reply.started":"2024-12-23T01:54:05.526860Z","shell.execute_reply":"2024-12-23T01:54:05.531089Z"}},"outputs":[{"name":"stdout","text":"Yes\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3-2. Implementation 実装","metadata":{}},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        debug : bool, default=False\n            Print debugging information.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-23T01:54:05.698340Z","iopub.execute_input":"2024-12-23T01:54:05.698589Z","iopub.status.idle":"2024-12-23T01:54:08.209460Z","shell.execute_reply.started":"2024-12-23T01:54:05.698568Z","shell.execute_reply":"2024-12-23T01:54:08.208555Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"p = '/kaggle/input/santa-2024/sample_submission.csv'\ndf = pd.read_csv(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:03:23.491614Z","iopub.execute_input":"2024-12-23T02:03:23.491925Z","iopub.status.idle":"2024-12-23T02:03:23.513109Z","shell.execute_reply.started":"2024-12-23T02:03:23.491900Z","shell.execute_reply":"2024-12-23T02:03:23.512239Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# initialize scorer\nscorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:08:00.065848Z","iopub.execute_input":"2024-12-23T02:08:00.066179Z","iopub.status.idle":"2024-12-23T02:10:34.132782Z","shell.execute_reply.started":"2024-12-23T02:08:00.066131Z","shell.execute_reply":"2024-12-23T02:10:34.132026Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ebf00304b7442aa8cb732ba8a359a4"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"t = \"\"\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\nreindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament\nsleigh yuletide beard carol cheer chimney decorations gifts grinch holiday holly jingle magi naughty nice nutcracker ornament polar workshop stocking\nsleigh of the magi yuletide cheer is unwrap gifts relax and eat cheer decorations carol sing chimney visit workshop grinch holiday holly jingle naughty nice nutcracker polar beard ornament stocking\nof and to in the as you that it we with from have not night season eggnog milk chocolate candy peppermint cookie fruitcake toy doll game puzzle snowglobe candle fireplace star angel wreath poinsettia greeting card wrapping paper bow wish dream believe wonder hope joy peace merry hohoho kaggle workshop\nfrom and and as and have the in is it of not that to with advent angel bake beard believe bow candy carol candle cheer chocolate chimney cookie decorations doll dream drive eat eggnog elf family fireplace fireplace chimney fruitcake game give gifts gingerbread grinch greeting card holly hohoho holiday hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night nutcracker ornament ornament of the night peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk we wish you cheer wonder workshop the workshop wreath wrapping paper yuletide\"\"\"\ndf['text'] = t.split('\\n')\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:10:44.929212Z","iopub.execute_input":"2024-12-23T02:10:44.929432Z","iopub.status.idle":"2024-12-23T02:10:44.945744Z","shell.execute_reply.started":"2024-12-23T02:10:44.929414Z","shell.execute_reply":"2024-12-23T02:10:44.945092Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:10:34.133799Z","iopub.execute_input":"2024-12-23T02:10:34.133998Z","iopub.status.idle":"2024-12-23T02:10:44.901746Z","shell.execute_reply.started":"2024-12-23T02:10:34.133980Z","shell.execute_reply":"2024-12-23T02:10:44.901057Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:10:44.902892Z","iopub.execute_input":"2024-12-23T02:10:44.903205Z","iopub.status.idle":"2024-12-23T02:10:44.910988Z","shell.execute_reply.started":"2024-12-23T02:10:44.903173Z","shell.execute_reply":"2024-12-23T02:10:44.910220Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   id                                               text       score\n0   0  reindeer mistletoe elf gingerbread family adve...  468.961335\n1   1  reindeer sleep walk the night and drive mistle...  421.728839\n2   2  sleigh yuletide beard carol cheer chimney deco...  297.479242\n3   3  sleigh of the magi yuletide cheer is unwrap gi...  195.810768\n4   4  of and to in the as you that it we with from h...   68.091304\n5   5  from and and as and have the in is it of not t...   33.426990","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>reindeer mistletoe elf gingerbread family adve...</td>\n      <td>468.961335</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>reindeer sleep walk the night and drive mistle...</td>\n      <td>421.728839</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>sleigh yuletide beard carol cheer chimney deco...</td>\n      <td>297.479242</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>sleigh of the magi yuletide cheer is unwrap gi...</td>\n      <td>195.810768</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>of and to in the as you that it we with from h...</td>\n      <td>68.091304</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>from and and as and have the in is it of not t...</td>\n      <td>33.426990</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(np.mean(df['score']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:10:44.911855Z","iopub.execute_input":"2024-12-23T02:10:44.912131Z","iopub.status.idle":"2024-12-23T02:10:44.928468Z","shell.execute_reply.started":"2024-12-23T02:10:44.912097Z","shell.execute_reply":"2024-12-23T02:10:44.927625Z"}},"outputs":[{"name":"stdout","text":"247.58307971111267\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}