{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":210460487,"sourceType":"kernelVersion"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook I combined publicly available solutions and improved id=4 a little. This was done by trying all possible rearrangements of the publicly best string, in the form of splitting it into 3 parts and concatenating them in some other order. I tried the same for smaller id's, it didn't improve the score, still need to do this for the last one\n\nFor further improvements:\n- it is a dubious technique, which may be used at every step of competition to try and verify that the answer is a «local» optimum\n- there are more available options, like doing swaps of elements, cyclic shifts of N elements, sorting or reversing subsegments, doing concatenation like it is done here, but for more than 3 parts\n- if done in the simplest form, the amount of strings to consider grows polinomially if we want to increase the number of parts we split into, so realistically trying more than 4-5 parts this way isn't possible\n- by adding constraints (for example not swapping words too far apart) one can speed up the search\n- adding some meaning behind the way we select possible candidates (like looking at the words / using greedy algorithm) may further improve the score, but then it’s not a dubious technique :)","metadata":{}},{"cell_type":"markdown","source":"# Metric Code for Batches","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = True,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str\n        Path to the serialized LLM.\n\n    clear_mem : bool\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n                \n            #quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n\n            quantization_config = transformers.BitsAndBytesConfig(\n                load_in_4bit = True,\n                bnb_4bit_quant_type = \"fp4\", #fp4 nf4\n                bnb_4bit_use_double_quant = False,\n                bnb_4bit_compute_dtype=torch.float16,\n            )\n            \n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n        #if not load_in_8bit:\n        #    self.model.to(DEVICE)  # Explicitly move the model to the device\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], batch_size: 32\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        verbose : bool, default=False\n            Display progress bar.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n\n        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n        for j in range(batches):\n            \n            a = j*batch_size\n            b = (j+1)*batch_size\n            input_batch = input_texts[a:b]\n        \n            with torch.no_grad():\n\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                    padding=True\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                label = model_inputs['input_ids']\n                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = label[..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                loss = loss.view(len(logits), -1)\n                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n                loss = torch.sum(loss, -1) / valid_length\n\n                loss_list += loss.cpu().tolist()\n\n                # Debug output\n                #print(f\"\\nProcessing: '{text}'\")\n                #print(f\"With special tokens: '{text_with_special}'\")\n                #print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                #print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                #print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                #print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                #print(f\"Individual losses: {loss.tolist()}\")\n                #print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        # print(\"\\nFinal perplexities:\")\n        # for text, perp in zip(input_texts, ppl):\n        #     print(f\"Text: '{text}'\")\n        #     print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-17T05:07:15.763630Z","iopub.execute_input":"2024-12-17T05:07:15.764002Z","iopub.status.idle":"2024-12-17T05:07:40.982385Z","shell.execute_reply.started":"2024-12-17T05:07:15.763965Z","shell.execute_reply":"2024-12-17T05:07:40.981378Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"p = '/kaggle/input/santa-2024/sample_submission.csv'\ndf = pd.read_csv(p)\nscorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:07:40.984007Z","iopub.execute_input":"2024-12-17T05:07:40.984460Z","iopub.status.idle":"2024-12-17T05:07:54.715873Z","shell.execute_reply.started":"2024-12-17T05:07:40.984409Z","shell.execute_reply":"2024-12-17T05:07:54.714898Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"t = \"\"\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\nreindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament\nmagi yuletide cheer grinch carol holiday holly jingle naughty nice nutcracker polar beard ornament stocking chimney sleigh workshop gifts decorations\nsleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax carol sing chimney visit grinch naughty nice polar beard workshop nutcracker ornament stocking\nfrom and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping paper\nfrom and and as and have the in is it of of not that the to we with you advent card the angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game give gifts gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\"\"\"\n\ndf['text'] = t.split('\\n')\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x, 1))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:07:54.717078Z","iopub.execute_input":"2024-12-17T05:07:54.717564Z","iopub.status.idle":"2024-12-17T05:08:51.502583Z","shell.execute_reply.started":"2024-12-17T05:07:54.717530Z","shell.execute_reply":"2024-12-17T05:08:51.501786Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"255.75636444802402\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    423.612476\n2    303.031473\n3    209.184454\n4     95.162854\n5     35.047809\nName: score, dtype: float64"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from itertools import combinations\n\ns = df['text'][4]\nprint(s)\nB = scorer.get_perplexity(s, 1)\nprint(B)\n\nwords = s.split()\n\nresult_strings = []\n# for i in range(1, len(words)):\n#     for j in range(i + 1, len(words)):\n#         result_strings.append(\" \".join(words[j:] + words[i:j] + words[:i]))\n#         result_strings.append(\" \".join(words[j:] + words[:i] + words[i:j]))\n#         result_strings.append(\" \".join(words[i:j] + words[:i] + words[j:]))\n#         result_strings.append(\" \".join(words[i:j] + words[j:] + words[:i]))\n#        result_strings.append(\" \".join(words[:i] + words[j:] + words[i:j]))\nL = len(result_strings)\nprint(L)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:08:51.504451Z","iopub.execute_input":"2024-12-17T05:08:51.504921Z","iopub.status.idle":"2024-12-17T05:08:53.092336Z","shell.execute_reply.started":"2024-12-17T05:08:51.504887Z","shell.execute_reply":"2024-12-17T05:08:53.091454Z"}},"outputs":[{"name":"stdout","text":"from and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping paper\n95.16285393578372\n0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nBATCH_SIZE = 64\n\nbest_score = 1e6\nbest_text = \"\"\n\n# ITERATE OVER ALL CANDIDATES\nperms = []\nfor i,p in enumerate(result_strings):\n    perms.append(p)\n    if (len(perms)==BATCH_SIZE) | (i==len(result_strings)-1): \n        p = scorer.get_perplexity(perms, batch_size=len(perms))\n        if np.min(p) < best_score:\n            best_score = np.min(p)\n            best_text = perms[ np.argmin(p) ]\n            print( f\"New best = {best_score} with '{best_text}'\" )\n        print(f\"Completed computing {i + 1} perplexities.\")\n        perms = []\nprint( f\"Best = {best_score} with '{best_text}'\" )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:08:53.093379Z","iopub.execute_input":"2024-12-17T05:08:53.093713Z","iopub.status.idle":"2024-12-17T05:08:53.099808Z","shell.execute_reply.started":"2024-12-17T05:08:53.093678Z","shell.execute_reply":"2024-12-17T05:08:53.099098Z"}},"outputs":[{"name":"stdout","text":"Best = 1000000.0 with ''\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nprint(\"Submission shape is\",df.shape)\ndf.to_csv(\"submission.csv\",index=False)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:08:53.100656Z","iopub.execute_input":"2024-12-17T05:08:53.100886Z","iopub.status.idle":"2024-12-17T05:08:53.118845Z","shell.execute_reply.started":"2024-12-17T05:08:53.100861Z","shell.execute_reply":"2024-12-17T05:08:53.118155Z"}},"outputs":[{"name":"stdout","text":"Submission shape is (6, 3)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   id                                               text       score\n0   0  reindeer mistletoe elf gingerbread family adve...  468.499121\n1   1  reindeer sleep walk the night and drive mistle...  423.612476\n2   2  magi yuletide cheer grinch carol holiday holly...  303.031473\n3   3  sleigh of the magi yuletide cheer is unwrap gi...  209.184454\n4   4  from and as have in not it of that the to we w...   95.162854\n5   5  from and and as and have the in is it of of no...   35.047809","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>reindeer mistletoe elf gingerbread family adve...</td>\n      <td>468.499121</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>reindeer sleep walk the night and drive mistle...</td>\n      <td>423.612476</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>magi yuletide cheer grinch carol holiday holly...</td>\n      <td>303.031473</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>sleigh of the magi yuletide cheer is unwrap gi...</td>\n      <td>209.184454</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>from and as have in not it of that the to we w...</td>\n      <td>95.162854</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>from and and as and have the in is it of of no...</td>\n      <td>35.047809</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}