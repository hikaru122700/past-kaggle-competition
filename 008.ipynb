{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm.auto import tqdm\nimport random, pickle, math, warnings\nimport itertools,  multiprocessing, json\n#warnings.simplefilter('ignore')\nprint(\"CPU Count: \", multiprocessing.cpu_count())\n\np = '/kaggle/input/santa-2024/sample_submission.csv'\ndf = pd.read_csv(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:00:36.367330Z","iopub.execute_input":"2024-12-18T08:00:36.367574Z","iopub.status.idle":"2024-12-18T08:00:38.138946Z","shell.execute_reply.started":"2024-12-18T08:00:36.367548Z","shell.execute_reply":"2024-12-18T08:00:38.138208Z"}},"outputs":[{"name":"stdout","text":"CPU Count:  96\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!pip3 install torch-xla\n#import torch_xla.core.xla_model as xm","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-18T08:00:38.140232Z","iopub.execute_input":"2024-12-18T08:00:38.140479Z","iopub.status.idle":"2024-12-18T08:00:38.143376Z","shell.execute_reply.started":"2024-12-18T08:00:38.140454Z","shell.execute_reply":"2024-12-18T08:00:38.142788Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#import tensorflow as tf\n\n#print(\"Tensorflow version \" + tf.__version__)\n#AUTO = tf.data.experimental.AUTOTUNE\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#print('Running on TPU ', tpu.master())\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n#tpu_strategy = tf.distribute.TPUStrategy(tpu)\n#print(\"REPLICAS: \", tpu_strategy.num_replicas_in_sync)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-18T08:00:38.144282Z","iopub.execute_input":"2024-12-18T08:00:38.144659Z","iopub.status.idle":"2024-12-18T08:00:38.158192Z","shell.execute_reply.started":"2024-12-18T08:00:38.144628Z","shell.execute_reply":"2024-12-18T08:00:38.157613Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import transformers, torch, os\nfrom math import exp\nfrom collections import Counter, OrderedDict\nfrom typing import List, Optional, Union\nimport torch.nn.functional as F\n\nDEVICE = torch.device('cpu')\n#DEVICE = xm.xla_device(tpu_strategy)\nMODEL_PATH = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n\n#https://www.kaggle.com/code/neibyr/batch-metric-with-cache\nclass LRUCache:\n    def __init__(self, capacity=10**11):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n    def get(self, key):\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return None\n    def set(self, key, value):\n        self.cache[key] = value\n        self.cache.move_to_end(key)\n        if len(self.cache) > self.capacity:\n            self.cache.popitem(last=False)\n    def __len__(self):\n        return len(self.cache)\n\nclass PerplexityCalculator:\n    def __init__(self, capacity=10**11):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", torch_dtype=torch.float32,)\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        self.model.eval()\n        #self.model.to(DEVICE)\n        self.cache = LRUCache(capacity=capacity)\n\n    #Add multiprocessing\n    def get_perplexity(self, input_texts, batch_size=128, use_cache=True,) -> Union[float, List[float]]:\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n        results = [None] * len(input_texts)\n        if use_cache:\n            text_to_process = []\n            for i, text in enumerate(input_texts):\n                cached_val = self.cache.get(text)\n                if cached_val is not None:\n                    results[i] = cached_val\n                else:\n                    text_to_process.append(text)\n        else:\n            text_to_process = input_texts.copy()\n        loss_list = []\n        batches = len(text_to_process)//batch_size + (len(text_to_process)%batch_size != 0)\n        pbar = range(batches)\n        for j in pbar:\n            a = j*batch_size\n            b = (j+1)*batch_size\n            input_batch = text_to_process[a:b]\n            with torch.no_grad():\n                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n                model_inputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False,)\n                #model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n                logits = self.model(**model_inputs, use_cache=True)['logits']\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1))\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n        ppl = [exp(i) for i in loss_list]\n        index_ppl = 0\n        for index_el, el in enumerate(results):\n            if el is None:\n                results[index_el] = ppl[index_ppl]\n                self.cache.set(text_to_process[index_ppl], ppl[index_ppl])\n                index_ppl += 1\n        return results[0] if single_input else results\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n     # define your model normally\nscorer = PerplexityCalculator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:00:38.159625Z","iopub.execute_input":"2024-12-18T08:00:38.159844Z","iopub.status.idle":"2024-12-18T08:01:11.388437Z","shell.execute_reply.started":"2024-12-18T08:00:38.159822Z","shell.execute_reply":"2024-12-18T08:01:11.387412Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"t = \"\"\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\nreindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament\nmagi yuletide cheer grinch carol holiday holly jingle naughty nice nutcracker polar beard ornament stocking chimney sleigh workshop gifts decorations\nsleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax carol sing chimney visit grinch naughty nice polar beard workshop nutcracker ornament stocking\nfrom and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping paper\nfrom and and as and have the in is it of not that the to we with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game give gifts gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\"\"\"\n\ndf['text'] = t.split('\\n')\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:01:11.389608Z","iopub.execute_input":"2024-12-18T08:01:11.390067Z","iopub.status.idle":"2024-12-18T08:02:02.116067Z","shell.execute_reply.started":"2024-12-18T08:01:11.390038Z","shell.execute_reply":"2024-12-18T08:02:02.115216Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"255.69357441222382\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    423.612476\n2    303.031473\n3    209.184454\n4     95.162854\n5     34.671069\nName: score, dtype: float64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def part_perm_brutem(st, start=0, end=3, skips=1):\n    bestt = st\n    best = scorer.get_perplexity(st)\n    st = st.split(' ')\n    part = st[start:end]\n    if start>0:\n        st1 =  ' '.join(st[:start]) + ' '\n    else:\n        st1 = ''\n    if end<len(st): \n        st2 =  ' ' + ' '.join(st[end:])\n    else: \n        st2 = ''\n    p = list(itertools.permutations(part))\n    for i in range(0, len(p), skips): #removed tqdm\n        t =  st1 + ' '.join(list(p[i])) + st2\n        s =  scorer.get_perplexity(t)\n        if s < best:\n            print(\"New Score: \", s, t)\n            best = s\n            bestt = t\n    return bestt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:02:02.117204Z","iopub.execute_input":"2024-12-18T08:02:02.117583Z","iopub.status.idle":"2024-12-18T08:02:02.124354Z","shell.execute_reply.started":"2024-12-18T08:02:02.117547Z","shell.execute_reply":"2024-12-18T08:02:02.123538Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"for i in range(5,6):\n    bestt = df['text'][i]\n    l = len(df['text'][i].split(' '))\n    for p in range(2, 3):\n        #for start in tqdm(range(0,l-p+1)):\n        for start in tqdm(range(45,50)):\n            bestt = part_perm_brutem(bestt, start, start+p, 1)\n            df.at[i, 'text'] = bestt\n        df.to_csv(\"submission.csv\", index=False)\n\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:02:02.125409Z","iopub.execute_input":"2024-12-18T08:02:02.125697Z","iopub.status.idle":"2024-12-18T08:02:14.217847Z","shell.execute_reply.started":"2024-12-18T08:02:02.125667Z","shell.execute_reply":"2024-12-18T08:02:14.216751Z"}},"outputs":[{"name":"stderr","text":" 20%|██        | 1/5 [00:02<00:10,  2.67s/it]","output_type":"stream"},{"name":"stdout","text":"New Score:  34.6528466648271 from and and as and have the in is it of not that the to we with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game gifts give gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5/5 [00:12<00:00,  2.41s/it]","output_type":"stream"},{"name":"stdout","text":"255.69053737218272\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    423.612476\n2    303.031473\n3    209.184454\n4     95.162854\n5     34.652847\nName: score, dtype: float64"},"metadata":{}}],"execution_count":7}]}