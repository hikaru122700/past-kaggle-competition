{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"このノートブックは以下のノートブックを参考にして作成されました。\n\nこの場を借りてノートブックを共有してくださったことに感謝したいと思います。\n\nhttps://www.kaggle.com/code/utm529fg/eng-tutorial-3-beam-search\n\nまた、以下のノートブックを利用してid4以外の答えを作成しています。ありがとうございます。\n\nhttps://www.kaggle.com/code/jazivxt/diminutive-effort-tpu","metadata":{}},{"cell_type":"markdown","source":"This notebook was created with reference to the following notebook:\n\nhttps://www.kaggle.com/code/utm529fg/eng-tutorial-3-beam-search\n\nI would like to take this opportunity to express my gratitude to the author for sharing their notebook.\n\nAdditionally, I used the following notebook to generate answers other than id4. Thank you very much.\n\nhttps://www.kaggle.com/code/jazivxt/diminutive-effort-tpu","metadata":{}},{"cell_type":"markdown","source":"このノートブックはGPU環境下で実行に3時間程度かかります。実行される際は注意してください。\n\n元ノートブックからの変更点として、ビーム幅を可変にしています。\n\nこれにより、重要な局面で多くの情報を残すことができます。","metadata":{}},{"cell_type":"markdown","source":"This notebook takes approximately 3 hours to run in a GPU environment. Please be cautious when executing it.\n\nAs a modification from the original notebook, the beam width has been made adjustable.\n\nThis allows for retaining more information during critical stages.","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"print(\"Yes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:29:21.062269Z","iopub.execute_input":"2024-12-20T08:29:21.062572Z","iopub.status.idle":"2024-12-20T08:29:21.074132Z","shell.execute_reply.started":"2024-12-20T08:29:21.062524Z","shell.execute_reply":"2024-12-20T08:29:21.073416Z"}},"outputs":[{"name":"stdout","text":"Yes\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3-2. Implementation 実装","metadata":{}},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        debug : bool, default=False\n            Print debugging information.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-20T08:29:21.237956Z","iopub.execute_input":"2024-12-20T08:29:21.238224Z","iopub.status.idle":"2024-12-20T08:29:45.250747Z","shell.execute_reply.started":"2024-12-20T08:29:21.238197Z","shell.execute_reply":"2024-12-20T08:29:45.249796Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/santa-2024/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:29:45.252412Z","iopub.execute_input":"2024-12-20T08:29:45.252793Z","iopub.status.idle":"2024-12-20T08:29:45.262884Z","shell.execute_reply.started":"2024-12-20T08:29:45.252765Z","shell.execute_reply":"2024-12-20T08:29:45.262105Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# initialize scorer\nscorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:29:45.263880Z","iopub.execute_input":"2024-12-20T08:29:45.264157Z","iopub.status.idle":"2024-12-20T08:30:13.984039Z","shell.execute_reply.started":"2024-12-20T08:29:45.264130Z","shell.execute_reply":"2024-12-20T08:30:13.983081Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1734683402.135623      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE1220 08:30:02.175654080      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-12-20T08:30:02.17562356+00:00\", grpc_status:2}\nLoading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.21it/s]\nW1220 09:43:04.406000 136369790703296 torch/_inductor/compile_worker/subproc_pool.py:126] SubprocPool unclean exit\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import random\nimport time\nclass BeamSearch():\n    def __init__(self, beam_width):\n        self.beam_width = beam_width\n\n    def _make_candidates_adding_word(self, intermediate_solution, words):\n        '''Make candidate text so that word duplication does not occur'''\n        candidates = []\n        not_used_words = words.copy()\n        for used_word in intermediate_solution.split():\n            not_used_words.remove(used_word)\n        \n        candidates += [(intermediate_solution + ' ' + word).lstrip() for word in not_used_words]\n        candidates += [(word + ' ' + intermediate_solution).rstrip() for word in not_used_words]\n\n        candidates = list(set(candidates)) # drop duplicate\n        \n        return candidates\n    def solve(self, text):\n        words = text.split()\n        intermediate_solutions = ['']\n        eval_cnt = 0\n\n        while len(intermediate_solutions[0].split()) < len(words):\n            start_time = time.time()\n            candidates = []\n            for intermediate_solution in intermediate_solutions:\n                candidates += self._make_candidates_adding_word(intermediate_solution, words)\n            \n            # 重複を排除\n            candidates = list(set(candidates))\n            \n            scores = scorer.get_perplexity(candidates)\n            eval_cnt += len(scores)\n            \n            # スコアと候補をペアにする\n            scored_candidates = list(zip(scores, candidates))\n            \n            # スコアでソート（低い順）\n            scored_candidates.sort(key=lambda x: x[0])\n            \n            # ビーム幅に基づいて上位k個を選択、重複を避ける\n            new_intermediate_solutions = []\n            seen = set()\n            for score, candidate in scored_candidates:\n                if candidate not in seen:\n                    new_intermediate_solutions.append(candidate)\n                    seen.add(candidate)\n                if score < 1.1*scored_candidates[0][0] and len(new_intermediate_solutions) < 7:\n                    continue\n                if len(new_intermediate_solutions) >= self.beam_width:\n                    break\n            \n            intermediate_solutions = new_intermediate_solutions\n            print(f\"score, intermediate_solutions:{min(scorer.get_perplexity(intermediate_solutions))}, {intermediate_solutions}\")\n            print(f\"remain_time:{len(new_intermediate_solutions[0].split())}/{len(words)}, {(len(words)-len(new_intermediate_solutions[0].split()))*(time.time()-start_time)}\\n\")\n\n        # 最終的なベストスコアとソリューションを取得\n        final_scores = scorer.get_perplexity(intermediate_solutions)\n        best_idx = np.argmin(final_scores)\n        best_score = final_scores[best_idx]\n        best_solution = intermediate_solutions[best_idx]\n\n        return best_score, best_solution, eval_cnt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:13.985195Z","iopub.execute_input":"2024-12-20T08:30:13.985677Z","iopub.status.idle":"2024-12-20T08:30:13.995817Z","shell.execute_reply.started":"2024-12-20T08:30:13.985649Z","shell.execute_reply":"2024-12-20T08:30:13.995056Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"params = {\n    'beam_width': 2,\n}\noptimizer = BeamSearch(**params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:13.997742Z","iopub.execute_input":"2024-12-20T08:30:13.997983Z","iopub.status.idle":"2024-12-20T08:30:14.014031Z","shell.execute_reply.started":"2024-12-20T08:30:13.997960Z","shell.execute_reply":"2024-12-20T08:30:14.013072Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"sample_submission.loc[0][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.014975Z","iopub.execute_input":"2024-12-20T08:30:14.015199Z","iopub.status.idle":"2024-12-20T08:30:14.028289Z","shell.execute_reply.started":"2024-12-20T08:30:14.015176Z","shell.execute_reply":"2024-12-20T08:30:14.027573Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import time\nstart_time = time.time()\nfrom tqdm import tqdm\nscores = []\nsolutions = []\neval_cnts = []\n# for text in tqdm(sample_submission['text']):\n#text = sample_submission.loc[4][\"text\"]\n#score, solution, eval_cnt = optimizer.solve(text)\n#scores.append(score)\n#solutions.append(solution)\n#eval_cnts.append(eval_cnt)\n\n#print(scores)\n#print(solutions)\n#print(eval_cnts)\n#print(f\"execute_time:{time.time()-start_time}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.029304Z","iopub.execute_input":"2024-12-20T08:30:14.029785Z","iopub.status.idle":"2024-12-20T08:30:14.038507Z","shell.execute_reply.started":"2024-12-20T08:30:14.029759Z","shell.execute_reply":"2024-12-20T08:30:14.037749Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"solutions = ['the of to and as in not it have from we with you that candle night season wonder wish hope dream believe joy peace merry hohoho chocolate cookie milk eggnog fruitcake peppermint candy toy doll game puzzle snowglobe fireplace angel star wreath poinsettia greeting card wrapping paper bow kaggle workshop add Codeadd Markdown']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.039566Z","iopub.execute_input":"2024-12-20T08:30:14.040237Z","iopub.status.idle":"2024-12-20T08:30:14.048818Z","shell.execute_reply.started":"2024-12-20T08:30:14.040211Z","shell.execute_reply":"2024-12-20T08:30:14.047999Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"most_best_text = \"from and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping paper\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.049818Z","iopub.execute_input":"2024-12-20T08:30:14.050069Z","iopub.status.idle":"2024-12-20T08:30:14.059615Z","shell.execute_reply.started":"2024-12-20T08:30:14.050042Z","shell.execute_reply":"2024-12-20T08:30:14.058871Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import math\nclass SimulatedAnnealing:\n    def __init__(self, start_temp, end_temp, max_iterations, random_state, alpha):\n        self.start_temp = start_temp\n        self.end_temp = end_temp\n        self.max_iterations = max_iterations\n        random.seed(random_state)\n        self.alpha = alpha  # 指数減少率\n\n    def _generate_neighbor(self, solution):\n        \"\"\"\n        \n        近傍解を生成（ランダムな2単語を入れ替え）\n        \"\"\"\n        r = random.choice(range(4))\n        if r == 0:\n            # Generate a neighborhood solution by swapping two random words.\n            neighbor = solution.copy()\n            i, j = random.sample(range(len(neighbor)), 2)\n            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n            return neighbor\n        elif r==1:\n            # \n            shift = solution.copy()\n            extract, insert = random.sample(range(len(shift) - 1), 2)\n            shift_words = shift[extract:extract+1]\n            shift = shift[:extract] + shift[extract+1:]\n            shift = shift[:insert] + shift_words + shift[insert:]\n            return shift\n        elif r == 2:\n            neighbor = solution.copy()\n            if len(neighbor) < 2:\n                return neighbor\n            start = random.randint(0, len(neighbor) - 2)\n            end = random.randint(start + 1, len(neighbor) - 1)\n            neighbor[start:end+1] = reversed(neighbor[start:end+1])\n            return neighbor\n        elif r == 3:\n            block_size = random.choice(range(2, 5))\n            #  block_size = 2\n            neighbor = solution.copy()\n            if len(neighbor) < block_size + 1:\n                return neighbor\n            start = random.randint(0, len(neighbor) - block_size)\n            block = neighbor[start:start + block_size]\n            insert_pos = random.randint(0, len(neighbor) - block_size)\n            # ブロックを削除\n            del neighbor[start:start + block_size]\n            # 挿入位置にブロックを挿入\n            neighbor = neighbor[:insert_pos] + block + neighbor[insert_pos:]\n            return neighbor\n\n    def _acceptance_probability(self, current_energy, new_energy, temperature):\n        \"\"\"\n        Calculate the probability of accepting a new solution.\n        新しい解を受け入れる確率を計算\n        \"\"\"\n        if new_energy < current_energy:\n            return 1.0\n        return exp((current_energy - new_energy) / temperature)\n\n    def _lower_temperature(self, temperature, iteration):\n        \"\"\"\n        温度を指数関数的に減少させる。\n        \"\"\"\n        # 指数関数的減少\n        new_temperature = self.start_temp * (self.alpha ** iteration)\n        \n        return new_temperature\n    def solve(self, text):\n        current_solution = text.split()\n        current_energy = scorer.get_perplexity(' '.join(current_solution))\n\n        best_solution = current_solution.copy()\n        best_energy = current_energy\n\n        temperature = self.start_temp\n        \n        log_energies = [current_energy]\n        start_time = time.time()\n        spend_minute = 0\n        for iteration in range(self.max_iterations):\n            # generate neighbor\n            new_solution = self._generate_neighbor(current_solution)\n            new_energy = scorer.get_perplexity(' '.join(new_solution))\n\n            # calculation of acceptance probability\n            acceptance = self._acceptance_probability(\n                current_energy, new_energy, temperature\n            )\n            \n            # update current solution\n            if acceptance > random.random():\n                current_solution = new_solution\n                current_energy = new_energy\n                \n            # update best solution\n            if new_energy < best_energy:\n                best_solution = new_solution.copy()\n                best_energy = new_energy\n                print(f\"best_score:{best_energy}\")\n\n            # lower the temperature\n            temperature = self._lower_temperature(temperature, iteration)\n            # print(\"temperature:\", temperature)\n\n            # log\n            log_energies.append(current_energy)\n            current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n            if time.time()-start_time-60*spend_minute > 60:\n                spend_minute += 1\n                print(f\"{current_time} progress:{iteration/self.max_iterations}, current_score:{current_energy}, temperature:{temperature}, best_score:{best_energy}\")\n                \n            if temperature <= self.end_temp:\n                print(\"最低温度に到達しました。終了します。\")\n                break\n                \n                \n        print(f\"execution time:{time.time()-start_time}\")\n        return ' '.join(best_solution), best_energy, log_energies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.060565Z","iopub.execute_input":"2024-12-20T08:30:14.060857Z","iopub.status.idle":"2024-12-20T08:30:14.076814Z","shell.execute_reply.started":"2024-12-20T08:30:14.060832Z","shell.execute_reply":"2024-12-20T08:30:14.076039Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"sa_params = {\n    'start_temp': 10,          # 最初の温度\n    'end_temp': 0.1,             # 最後の温度　線形に下げていく\n    'max_iterations': 50000,  # 反復回数100000回で4時間程度か\n    'random_state': 42,\n    \"alpha\":0.9999\n}\nsa_optimizer = SimulatedAnnealing(**sa_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.077780Z","iopub.execute_input":"2024-12-20T08:30:14.078026Z","iopub.status.idle":"2024-12-20T08:30:14.091771Z","shell.execute_reply.started":"2024-12-20T08:30:14.078003Z","shell.execute_reply":"2024-12-20T08:30:14.090957Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"solution, score_2, log_scores = sa_optimizer.solve(solutions[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:30:14.092732Z","iopub.execute_input":"2024-12-20T08:30:14.092997Z","iopub.status.idle":"2024-12-20T09:27:02.959470Z","shell.execute_reply.started":"2024-12-20T08:30:14.092972Z","shell.execute_reply":"2024-12-20T09:27:02.957406Z"}},"outputs":[{"name":"stdout","text":"2024-12-20 08:31:58 progress:0.00078, current_score:171.32883614145945, temperature:9.961074008692197, best_score:137.0704508268321\n2024-12-20 08:32:59 progress:0.00156, current_score:198.49830167178683, temperature:9.922299540664326, best_score:137.0704508268321\n2024-12-20 08:33:58 progress:0.00234, current_score:182.34824850348582, temperature:9.883676006096994, best_score:137.0704508268321\n2024-12-20 08:34:59 progress:0.00314, current_score:206.3970125551687, temperature:9.844218297185002, best_score:137.0704508268321\n2024-12-20 08:35:58 progress:0.00392, current_score:206.3970125551687, temperature:9.805898701598167, best_score:137.0704508268321\n2024-12-20 08:36:58 progress:0.0047, current_score:206.3970125551687, temperature:9.767728268835809, best_score:137.0704508268321\n2024-12-20 08:37:59 progress:0.0055, current_score:202.807604129897, temperature:9.728733447625013, best_score:137.0704508268321\n2024-12-20 08:38:58 progress:0.00628, current_score:223.56640855258547, temperature:9.690863388263196, best_score:137.0704508268321\n2024-12-20 08:39:58 progress:0.00706, current_score:243.41543026209388, temperature:9.653140741861533, best_score:137.0704508268321\n2024-12-20 08:40:58 progress:0.00784, current_score:262.7302634299032, temperature:9.615564934600464, best_score:137.0704508268321\n2024-12-20 08:41:58 progress:0.00862, current_score:245.8325450105444, temperature:9.578135394894076, best_score:137.0704508268321\n2024-12-20 08:42:59 progress:0.00942, current_score:312.64729516835126, temperature:9.539897468226078, best_score:137.0704508268321\n2024-12-20 08:43:58 progress:0.0102, current_score:309.262362797279, temperature:9.50276247163353, best_score:137.0704508268321\n2024-12-20 08:44:58 progress:0.01098, current_score:285.45712281187707, temperature:9.465772026696438, best_score:137.0704508268321\n2024-12-20 08:45:58 progress:0.01176, current_score:271.25576358336946, temperature:9.428925570733156, best_score:137.0704508268321\n2024-12-20 08:46:58 progress:0.01254, current_score:310.77123541211625, temperature:9.392222543252327, best_score:137.0704508268321\n2024-12-20 08:47:58 progress:0.01332, current_score:346.2916904875766, temperature:9.35566238594437, best_score:137.0704508268321\n2024-12-20 08:48:58 progress:0.01412, current_score:383.25878566139596, temperature:9.318312618218702, best_score:137.0704508268321\n2024-12-20 08:49:58 progress:0.0149, current_score:417.1937487895002, temperature:9.282040162620685, best_score:137.0704508268321\n2024-12-20 08:50:58 progress:0.01568, current_score:417.1967328023958, temperature:9.245908901151802, best_score:137.0704508268321\n2024-12-20 08:51:59 progress:0.01648, current_score:417.1967328023958, temperature:9.208997292371485, best_score:137.0704508268321\n2024-12-20 08:52:58 progress:0.01726, current_score:417.07758781084294, temperature:9.17315035751584, best_score:137.0704508268321\n2024-12-20 08:53:58 progress:0.01804, current_score:415.30181771378227, temperature:9.13744296040766, best_score:137.0704508268321\n2024-12-20 08:54:58 progress:0.01882, current_score:388.58021360843253, temperature:9.101874557882422, best_score:137.0704508268321\n2024-12-20 08:55:58 progress:0.0196, current_score:405.6554408074029, temperature:9.066444608889938, best_score:137.0704508268321\n2024-12-20 08:56:58 progress:0.02038, current_score:395.0523198178425, temperature:9.031152574486107, best_score:137.0704508268321\n2024-12-20 08:57:59 progress:0.02118, current_score:444.4423518697536, temperature:8.995098318032936, best_score:137.0704508268321\n2024-12-20 08:58:59 progress:0.02196, current_score:430.5401522443981, temperature:8.960084006138878, best_score:137.0704508268321\n2024-12-20 08:59:59 progress:0.02274, current_score:443.1284454254797, temperature:8.925205990924866, best_score:137.0704508268321\n2024-12-20 09:00:58 progress:0.02352, current_score:444.4290006927271, temperature:8.890463741842556, best_score:137.0704508268321\n2024-12-20 09:01:58 progress:0.0243, current_score:380.0935055675566, temperature:8.855856730408826, best_score:137.0704508268321\n2024-12-20 09:02:59 progress:0.0251, current_score:295.909017117186, temperature:8.820502291754703, best_score:137.0704508268321\n2024-12-20 09:03:58 progress:0.02588, current_score:288.12487338444345, temperature:8.786167612200774, best_score:137.0704508268321\n2024-12-20 09:04:59 progress:0.02668, current_score:343.05265526629285, temperature:8.751091387132252, best_score:137.0704508268321\n2024-12-20 09:05:59 progress:0.02746, current_score:365.4940081770745, temperature:8.717026896405324, best_score:137.0704508268321\n2024-12-20 09:06:59 progress:0.02824, current_score:353.24748549992216, temperature:8.683095005085388, best_score:137.0704508268321\n2024-12-20 09:07:58 progress:0.029, current_score:354.7110655959942, temperature:8.650160213037415, best_score:137.0704508268321\n2024-12-20 09:08:58 progress:0.02978, current_score:411.7232539404926, temperature:8.616488606911034, best_score:137.0704508268321\n2024-12-20 09:09:58 progress:0.03056, current_score:434.10607537444156, temperature:8.582948070849396, best_score:137.0704508268321\n2024-12-20 09:10:59 progress:0.03136, current_score:444.69440407543544, temperature:8.54868314083981, best_score:137.0704508268321\n2024-12-20 09:11:58 progress:0.03214, current_score:425.6029422548346, temperature:8.51540654427646, best_score:137.0704508268321\n2024-12-20 09:12:58 progress:0.03294, current_score:407.9990316626105, temperature:8.481411254215955, best_score:137.0704508268321\n2024-12-20 09:13:58 progress:0.03372, current_score:380.90052866252904, temperature:8.448396520140005, best_score:137.0704508268321\n2024-12-20 09:14:59 progress:0.03452, current_score:371.56742120542174, temperature:8.414668748159302, best_score:137.0704508268321\n2024-12-20 09:15:58 progress:0.0353, current_score:366.03083933581905, temperature:8.381913815904413, best_score:137.0704508268321\n2024-12-20 09:16:59 progress:0.0361, current_score:364.48370517558914, temperature:8.348451456831803, best_score:137.0704508268321\n2024-12-20 09:17:59 progress:0.03688, current_score:409.4238622721817, temperature:8.315954281947578, best_score:137.0704508268321\n2024-12-20 09:18:59 progress:0.03766, current_score:408.76627561157267, temperature:8.28358360553806, best_score:137.0704508268321\n2024-12-20 09:19:58 progress:0.03844, current_score:385.38201030399597, temperature:8.251338935195399, best_score:137.0704508268321\n2024-12-20 09:20:58 progress:0.03922, current_score:387.97719413996776, temperature:8.219219780428483, best_score:137.0704508268321\n2024-12-20 09:21:59 progress:0.04002, current_score:379.1681211619491, temperature:8.18640693009023, best_score:137.0704508268321\n2024-12-20 09:22:59 progress:0.0408, current_score:382.67862202069887, temperature:8.154540529589948, best_score:137.0704508268321\n2024-12-20 09:23:58 progress:0.04158, current_score:371.4917742624799, temperature:8.122798172212553, best_score:137.0704508268321\n2024-12-20 09:24:58 progress:0.04236, current_score:390.12655840814284, temperature:8.091179375107895, best_score:137.0704508268321\n2024-12-20 09:25:59 progress:0.04316, current_score:381.1518023745988, temperature:8.058877688939633, best_score:137.0704508268321\n2024-12-20 09:26:58 progress:0.04394, current_score:375.414940179047, temperature:8.027507708652603, best_score:137.0704508268321\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m solution, score_2, log_scores \u001b[38;5;241m=\u001b[39m \u001b[43msa_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 86\u001b[0m, in \u001b[0;36mSimulatedAnnealing.solve\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iterations):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# generate neighbor\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_neighbor(current_solution)\n\u001b[0;32m---> 86\u001b[0m     new_energy \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_solution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# calculation of acceptance probability\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     acceptance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acceptance_probability(\n\u001b[1;32m     90\u001b[0m         current_energy, new_energy, temperature\n\u001b[1;32m     91\u001b[0m     )\n","Cell \u001b[0;32mIn[2], line 230\u001b[0m, in \u001b[0;36mPerplexityCalculator.get_perplexity\u001b[0;34m(self, input_texts, debug)\u001b[0m\n\u001b[1;32m    227\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Get model output\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Shift logits and labels for calculating loss\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:1049\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:837\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    826\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    827\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    828\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m         cache_position,\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:567\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    565\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    566\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 567\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n\u001b[1;32m    569\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:86\u001b[0m, in \u001b[0;36mGemma2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(log_scores)\n\n# plt.title(f'id = {4}, num of word = {len(text.split())}')\nplt.xlabel('sa_iteration')\nplt.ylabel('score')\nplt.legend()\nplt.show()\n\nprint(f'SA Score: {score_2}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.960327Z","iopub.status.idle":"2024-12-20T09:27:02.960729Z","shell.execute_reply.started":"2024-12-20T09:27:02.960509Z","shell.execute_reply":"2024-12-20T09:27:02.960527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(scorer.get_perplexity(solution), solution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.962217Z","iopub.status.idle":"2024-12-20T09:27:02.962583Z","shell.execute_reply.started":"2024-12-20T09:27:02.962414Z","shell.execute_reply":"2024-12-20T09:27:02.962431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(scorer.get_perplexity(most_best_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.964366Z","iopub.status.idle":"2024-12-20T09:27:02.964720Z","shell.execute_reply.started":"2024-12-20T09:27:02.964533Z","shell.execute_reply":"2024-12-20T09:27:02.964548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scorer.get_perplexity(' '.join(most_best_text.split()[:50]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.965833Z","iopub.status.idle":"2024-12-20T09:27:02.966159Z","shell.execute_reply.started":"2024-12-20T09:27:02.965997Z","shell.execute_reply":"2024-12-20T09:27:02.966013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p = '/kaggle/input/santa-2024/sample_submission.csv'\ndf = pd.read_csv(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.966937Z","iopub.status.idle":"2024-12-20T09:27:02.967251Z","shell.execute_reply.started":"2024-12-20T09:27:02.967099Z","shell.execute_reply":"2024-12-20T09:27:02.967114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t = \"\"\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\nreindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament\nmagi yuletide cheer grinch carol holiday holly jingle naughty nice nutcracker polar beard ornament stocking chimney sleigh workshop gifts decorations\nsleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax carol sing chimney visit grinch naughty nice polar beard workshop nutcracker ornament stocking\nfrom and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping paper\nfrom and and as and have the in is it of not that the to we with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game give gifts gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\"\"\"\n\ndf['text'] = t.split('\\n')\ndf.loc[4, \"text\"] = solution\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.968409Z","iopub.status.idle":"2024-12-20T09:27:02.968761Z","shell.execute_reply.started":"2024-12-20T09:27:02.968579Z","shell.execute_reply":"2024-12-20T09:27:02.968595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.970074Z","iopub.status.idle":"2024-12-20T09:27:02.970404Z","shell.execute_reply.started":"2024-12-20T09:27:02.970239Z","shell.execute_reply":"2024-12-20T09:27:02.970254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"10**20*0.9999**20000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T09:27:02.971449Z","iopub.status.idle":"2024-12-20T09:27:02.971784Z","shell.execute_reply.started":"2024-12-20T09:27:02.971603Z","shell.execute_reply":"2024-12-20T09:27:02.971618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}